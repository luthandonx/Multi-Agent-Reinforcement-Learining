{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luthandonx/Multi-Agent-Reinforcement-Learining/blob/PPO-on-Simple-Spread/Simple_Spread_PPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxFuGeqNtKV4"
      },
      "outputs": [],
      "source": [
        "!pip install pettingzoo[mpe]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43bkBdjQsTVP"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,LSTM,Input,Dropout\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "import numpy as np\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "np_config.enable_numpy_behavior()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOiV98mHrzeI"
      },
      "outputs": [],
      "source": [
        "class Replay:\n",
        "  def __init__(self,max_mem,n_weights,obsv_shape):\n",
        "    self.max_mem = max_mem\n",
        "    self.n_weights = n_weights\n",
        "    self.mem_counter = 0\n",
        "\n",
        "    self.state_memory = np.zeros((self.max_mem,obsv_shape))\n",
        "    self.action_memory = np.zeros((self.max_mem,n_weights))\n",
        "    self.reward_memory = np.zeros((self.max_mem))\n",
        "    self.next_state_memory = np.zeros((self.max_mem,obsv_shape))\n",
        "    self.terminal_memory = np.zeros(self.max_mem,dtype = bool)\n",
        "  \n",
        "  def store_transition(self,obsv,action,reward,done):\n",
        "    index = self.mem_counter % self.max_mem\n",
        "    self.state_memory[index] = obsv\n",
        "    self.action_memory[index] = action\n",
        "    self.reward_memory[index] = reward\n",
        "    self.terminal_memory[index] = done\n",
        "\n",
        "    if (self.mem_counter > 0):\n",
        "      self.next_state_memory[index-1] = obsv\n",
        "    self.mem_counter += 1\n",
        "\n",
        "    self.mem_counter += 1\n",
        "\n",
        "  def sample_buffer(self,batch_indices):\n",
        "    states = self.state_memory[batch_indices]\n",
        "    actions = self.action_memory[batch_indices]\n",
        "    rewards = self.reward_memory[batch_indices]\n",
        "    next_states = self.next_state_memory[batch_indices]\n",
        "    dones = self.terminal_memory[batch_indices]\n",
        "\n",
        "    return states,actions,rewards,next_states,dones\n",
        "\n",
        "  def return_indices(self,batch_size):\n",
        "    mem_size = min(batch_size,self.mem_counter) # we dont want to choose something outside our mem size\n",
        "    batch_indices = np.random.choice(mem_size,batch_size)\n",
        "    return batch_indices\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7NynacVsH3v"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ActorNet(keras.Model):\n",
        "  def __init__(self,layer_size,n_weights):\n",
        "    super(ActorNet,self).__init__()\n",
        "    self.layer_size = layer_size\n",
        "\n",
        "    self.layer_one = Dense(self.layer_size,activation = 'relu') #There might be an error with regards to the input shape \n",
        "    self.layer_two = Dense(self.layer_size,activation = 'relu')\n",
        "    self.policy = Dense(n_weights,activation = 'softmax')\n",
        "\n",
        "  def call(self,state):\n",
        "    v1 = self.layer_one(state)\n",
        "    v2 = self.layer_two(v1)\n",
        "    w = self.policy(v2)\n",
        "    return w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXdQiE9SsKpm"
      },
      "outputs": [],
      "source": [
        "class CriticNet(keras.Model):\n",
        "  def __init__(self,layer_size):\n",
        "    super(CriticNet,self).__init__()\n",
        "    self.layer_size = layer_size\n",
        "\n",
        "    self.layer_one = Dense(self.layer_size,activation = 'relu') #There might be an error with regards to the input shape \n",
        "    self.layer_two = Dense(self.layer_size,activation = 'relu')\n",
        "    self.action_value = Dense(1)\n",
        "\n",
        "  def call(self,inputs):\n",
        "    states,actions = inputs\n",
        "    input = tf.concat([states,actions],axis = 1)\n",
        "    v1 = self.layer_one(input)\n",
        "    v2 = self.layer_two(v1)\n",
        "    w = self.action_value(v2)\n",
        "    return w\n",
        "    print('w: ',w)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOg2n5GasQD-"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "  def __init__(self,alpha,beta,gamma,lambdah,layer_size ,mem_size,n_weights,obsv_shape, batch_size, file_name,tau): \n",
        "    \n",
        "    self.Actor = ActorNet(layer_size,n_weights)\n",
        "    self.Critic = CriticNet(layer_size)\n",
        "    self.Target_Actor = ActorNet(layer_size,n_weights)\n",
        "    self.Target_Critic = CriticNet(layer_size)\n",
        "\n",
        "    self.Target_Actor.compile(keras.optimizers.Adam(learning_rate = alpha))\n",
        "    self.Target_Critic.compile(keras.optimizers.Adam(learning_rate = beta))\n",
        "\n",
        "    self.max_mem = mem_size\n",
        "    self.n_weights = n_weights\n",
        "    self.obsv_shape = obsv_shape\n",
        "    self.Memory = Replay(self.max_mem, self.n_weights,self.obsv_shape )\n",
        "\n",
        "    self.file_name = file_name\n",
        "    self.tau = tau\n",
        "    self.batch_size = batch_size\n",
        "    self.gamma = gamma\n",
        "    self.learnCounter = 0\n",
        "    self.lambdah = lambdah # suggested value = 0.95\n",
        "    \n",
        "    self.clipping_value = 0.2\n",
        "    self.critic_discount = 0.5\n",
        "    self.entropy_beta = 0.001\n",
        "\n",
        "  def choose_action(self,observation):\n",
        "    state = tf.convert_to_tensor([observation])\n",
        "    probs = self.Actor(state)\n",
        "    action_probs = tfp.distributions.Categorical(probs = probs)\n",
        "    action = action_probs.sample()\n",
        "    self.action = action\n",
        "    action = action.numpy()[0]\n",
        "    return action\n",
        "  \n",
        "  def update_network_params(self):\n",
        "    tau = self.tau\n",
        "\n",
        "    weights = []\n",
        "    targets = self.Target_Actor.weights\n",
        "\n",
        "    for i,weight in enumerate(self.Actor.weights):\n",
        "      weights.append(weight * tau + targets[i] *(1-tau))\n",
        "    self.Target_Actor.set_weights(weights)\n",
        "\n",
        "    weights = []\n",
        "    targets = self.Target_Critic.weights  \n",
        "    for i,weight in enumerate(self.Critic.weights):\n",
        "      weights.append(weight * tau + targets[i] *(1-tau))\n",
        "    self.Target_Critic.set_weights(weights)\n",
        "\n",
        "\n",
        "  def store_transition(self,state,action,reward,done):\n",
        "    self.Memory.store_transition(state,action,reward,done)\n",
        "\n",
        "  def save_models(self):\n",
        "    print('Saving the models...')\n",
        "    self.Actor.save(self.file_name+' Actor')\n",
        "    self.Critic.save(self.file_name+' Critic')\n",
        "  \n",
        "  def load_models(self):\n",
        "    print('Loading the models......')\n",
        "    self.Actor = keras.models.load_model(self.file_name+' Actor')\n",
        "    self.Critic = keras.models.load_model(self.file_name+' Critic')\n",
        "    self.Target_Actor = keras.models.load_model(self.file_name+'Target Actor')\n",
        "    self.Target_Critic = keras.models.load_model(self.file_name+'Target Critic')\n",
        "\n",
        "  def save_models(self):\n",
        "    self.Actor.save(self.file_name + 'Actor')\n",
        "    self.Critic.save(self.file_name + 'Critic')\n",
        "    self.Target_Actor(self.file_name+ 'Target Actor')\n",
        "    self.Target_Critic(self.file_name+ 'Target Critic')\n",
        "\n",
        "  def get_advantages(self, rewards, state, state_, dones):\n",
        "      returns = []\n",
        "      target_actions = self.Target_Actor(state_)\n",
        "\n",
        "      target_probs = tfp.distributions.Categorical(probs = target_actions)\n",
        "      target_actions_1 = tf.transpose([target_probs.sample()])\n",
        "      target_actions_1 = tf.cast(target_actions_1,dtype = tf.float32)\n",
        "      values = self.Critic((state,target_actions_1))\n",
        "\n",
        "      action_probs = self.Actor(state)\n",
        "      action = tfp.distributions.Categorical(probs = action_probs)\n",
        "      action = tf.transpose([action.sample()])\n",
        "      action = tf.cast(action,tf.float32)\n",
        "      \n",
        "      values_ = self.Target_Critic((state,action))\n",
        "\n",
        "      gae = 0\n",
        "      for i in reversed(range(len(rewards))):\n",
        "        delta = rewards[i] +self.gamma*values_[i]*(1-dones[i]) -values[i]\n",
        "        gae = delta + self.gamma * self.lambdah *(1-dones[i])* gae\n",
        "        x = gae+values[i]\n",
        "        returns.insert(0,x)\n",
        "\n",
        "      return returns \n",
        "  \n",
        "  def learn(self):\n",
        "    if self.Memory.mem_counter < self.batch_size :\n",
        "      return\n",
        "\n",
        "    self.learnCounter += 1 \n",
        "    indices = self.Memory.return_indices(self.batch_size)\n",
        "    states,actions,rewards,next_states,dones = self.Memory.sample_buffer(indices)\n",
        "\n",
        "\n",
        "    tf_states = tf.convert_to_tensor(states, dtype = tf.float32)\n",
        "    tf_states_ = tf.convert_to_tensor(next_states, dtype = tf.float32)\n",
        "    tf_rewards = tf.convert_to_tensor(rewards, dtype = tf.float32)\n",
        "  \n",
        "    advantages = self.get_advantages(tf_rewards, tf_states, tf_states_, dones)\n",
        "   \n",
        "    actions_probs = self.Actor(tf_states)\n",
        "    actions = tfp.distributions.Categorical(probs = actions_probs)\n",
        "    actions = tf.transpose([actions.sample()])\n",
        "    actions = tf.cast(actions,dtype = tf.float32)\n",
        "    values = self.Critic((tf_states,actions))\n",
        "   \n",
        "\n",
        "    def custom_actor_loss(y_true,y_pred):\n",
        "      old_policy_probs = self.Actor(tf_states)\n",
        "      new_policy_probs = self.Target_Actor(tf_states_)\n",
        "      ratio = K.exp(K.log(new_policy_probs + 1e-10) - K.log(old_policy_probs + 1e-10))\n",
        "      part_1 = ratio * advantages\n",
        "      part_1 = ratio * advantages\n",
        "      part_2 = K.clip(ratio,min_value=1 - self.clipping_value,max_value_value= 1 + self.clipping_val)*advantages\n",
        "      actor_loss = -K.mean(K.minimum(part_1,part_2)) -  self.entropy_beta *K.mean(-(new_policy_probs * K.log(new_policy_probs + 1e-10)))\n",
        "      return actor_loss\n",
        "\n",
        "    \n",
        "    def custom_critic_loss(y_true,y_pred):\n",
        "      old_policy_probs = self.Actor(tf_states)\n",
        "      new_policy_probs = self.Target_Actor(tf_states_)\n",
        "      ratio = K.exp(K.log(new_policy_probs + 1e-10) - K.log(old_policy_probs + 1e-10))\n",
        "      part_1 = ratio * advantages\n",
        "      part_2 = K.clip(ratio,min_value=1 - self.clipping_value,max_value_value=1 + self.clipping_val)*advantages\n",
        "      actor_loss = -K.mean(K.minimum(part_1,part_2))\n",
        "      critic_loss = K.mean(K.square(rewards-values))\n",
        "      total_loss = self.critic_discount * critic_loss - self.entropy_beta *K.mean(-(new_policy_probs * K.log(new_policy_probs + 1e-10)))\n",
        "      return total_loss\n",
        "    \n",
        "    self.Actor.compile(optimizer = 'adam', loss = custom_actor_loss)\n",
        "    self.Critic.compile(optimizer = 'adam', loss = custom_critic_loss)\n",
        "\n",
        "    if (self.learnCounter % 100 == 0):\n",
        "      self.update_network_params()\n",
        "      self.learnCounter = 0\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EF65VugLtHNX"
      },
      "outputs": [],
      "source": [
        "from pettingzoo.mpe import simple_spread_v2\n",
        "\n",
        "env = simple_spread_v2.env(N=2, local_ratio=0.5, max_cycles=25, continuous_actions=False)\n",
        "env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gj_9vpEmtWwA"
      },
      "outputs": [],
      "source": [
        "agent_scores = {}\n",
        "policy_net = {}\n",
        "for a in env.agents:\n",
        "  obsv = env.observation_space(a).shape[0]\n",
        "  n_action = env.action_space(a).n\n",
        "  agent = Agent(alpha = 1e-5,beta = 1e-5,gamma = 0.65,lambdah = 0.2,layer_size = 256 ,mem_size = 10000,n_weights = n_action,obsv_shape = obsv, batch_size = 64, file_name = a,tau = 0.05)\n",
        "  policy_net[a] = agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "31_DcJCHtRLn"
      },
      "outputs": [],
      "source": [
        "\n",
        "agent_list = env.agents\n",
        "\n",
        "n_games = 1000\n",
        "best_score = -10000\n",
        "score_history = []\n",
        "model_loaded = False\n",
        "\n",
        "for i in range(n_games):\n",
        "  env.reset()\n",
        "  score = 0\n",
        "  for agent in env.agent_iter():\n",
        "    obsv,reward,done,trunc,_ = env.last()\n",
        "    if not (done or trunc):\n",
        "      action = policy_net[agent].choose_action(obsv)\n",
        "      score += reward\n",
        "      env.step(action)\n",
        "      policy_net[agent].store_transition(obsv,action,reward,done)\n",
        "      policy_net[agent].learn()\n",
        "    else:\n",
        "      action = None\n",
        "    env.step(action)\n",
        "    \n",
        "  score_history.append(score)    \n",
        "  avg_score = np.mean(score_history[-100:])\n",
        "\n",
        "  if (avg_score > best_score):\n",
        "    best_score = avg_score\n",
        "\n",
        "  print('episode: ',i ,'soore: ',score,' avg_score: ', avg_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBYy6oSHllhH"
      },
      "outputs": [],
      "source": [
        "print(score_history)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOqSXeTEXXBaGHdI/ZOgLh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}