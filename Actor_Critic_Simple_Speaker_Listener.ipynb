{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luthandonx/Multi-Agent-Reinforcement-Learining/blob/Simple-Listener-Speaker/Actor_Critic_Simple_Speaker_Listener.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leZ2Jd3yZygb"
      },
      "outputs": [],
      "source": [
        "!pip install pettingzoo[mpe]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbEIoqgmZ_4D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "class ActorCriticNetwork(keras.Model):\n",
        "  def __init__(self,n_actions, dims_1 = 256 , dims_2 = 256,\n",
        "               name = 'actor_critc', chkpt_dir = 'tmp/actor_critic'):\n",
        "    super(ActorCriticNetwork, self).__init__()\n",
        "    self.dims_1 = dims_1\n",
        "    self.dims_2 = dims_2\n",
        "    self.n_actions = n_actions\n",
        "    self.model_name = name\n",
        "    self.checkpoint_dir = chkpt_dir\n",
        "    self.checkpoint_file = os.path.join(self.checkpoint_dir, name+'_ac')\n",
        "\n",
        "    self.fc1 = Dense(self.dims_1,activation = 'relu')\n",
        "    self.fc2 = Dense(self.dims_2,activation = 'relu')\n",
        "    self.v = Dense(1,activation = None)\n",
        "    self.po = Dense(n_actions,activation = 'softmax')\n",
        "\n",
        "  def call(self,state):\n",
        "    value = self.fc1(state)\n",
        "    value = self.fc2(value)\n",
        "\n",
        "    v = self.v(value)\n",
        "    po = self.po(value)\n",
        "\n",
        "    return v,po"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaKCMHkNaFCa"
      },
      "outputs": [],
      "source": [
        "from re import A\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "\n",
        "class Agent:\n",
        "  def __init__(self, alpha = 1e-5, gamma = 0.95, n_actions = 4,name='agent'):\n",
        "    self.gamma = gamma\n",
        "    self.n_actions = n_actions\n",
        "    self.action = None\n",
        "    self.action_space = [i for i in range(self.n_actions)]\n",
        "    self.name = name\n",
        "    self.actor_critic = ActorCriticNetwork(n_actions=n_actions)\n",
        "    self.actor_critic.compile(optimizer = Adam(learning_rate = alpha))\n",
        "    self.fname = 'actor_critic_'+self.name\n",
        "\n",
        "   \n",
        "    self.reward_history = []\n",
        "    self.action_history = []\n",
        "    self.obsv_history = []\n",
        "    self.next_obsv_history = []\n",
        "\n",
        "  def choose_action(self,observation):\n",
        "    state = tf.convert_to_tensor([observation])\n",
        "    _,probs = self.actor_critic(state)\n",
        "\n",
        "    action_probs = tfp.distributions.Categorical(probs = probs)\n",
        "    action = action_probs.sample()\n",
        "\n",
        "    self.action = action\n",
        "\n",
        "    return action.numpy()[0]\n",
        "\n",
        "  def save_models(self):\n",
        "    print('saving model.....')\n",
        "    self.actor_critic.save(self.fname)\n",
        "  \n",
        "  def load_models(self):\n",
        "    #print('loading model....')\n",
        "    self.actor_critic = keras.models.load_model(self.fname)\n",
        "\n",
        "  def store_trans(self,action,reward,obsv,obsv_):\n",
        "      self.action_history.append(action)\n",
        "      self.reward_history.append(reward)\n",
        "      self.obsv_history.append(obsv)\n",
        "      self.next_obsv_history.append(obsv_)\n",
        "\n",
        "  def clear_memory(self):\n",
        "    self.reward_history = []\n",
        "    self.action_history = []\n",
        "    self.obsv_history = []\n",
        "    self.next_obsv_history = []\n",
        "\n",
        "  def learn(self,state,reward,state_,done):\n",
        "    state = tf.convert_to_tensor([state],dtype = tf.float32)\n",
        "    state_ = tf.convert_to_tensor([state_],dtype = tf.float32)\n",
        "    reward = tf.convert_to_tensor(reward,dtype = tf.float32)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      state_value, probs = self.actor_critic(state)\n",
        "      state_value_,_ = self.actor_critic(state_)\n",
        "      state_value = tf.squeeze(state_value)\n",
        "      state_value_ = tf.squeeze(state_value_)\n",
        "\n",
        "      action_probs = tfp.distributions.Categorical(probs = probs)\n",
        "      log_probs = action_probs.log_prob(self.action)\n",
        "\n",
        "      delta = reward + self.gamma*state_value_*(1-int(done)) - state_value\n",
        "\n",
        "      actor_loss = -log_probs*delta\n",
        "      critic_loss = delta**2\n",
        "\n",
        "      total_loss = actor_loss + critic_loss\n",
        "\n",
        "    gradients = tape.gradient(total_loss,self.actor_critic.trainable_variables)\n",
        "    self.actor_critic.optimizer.apply_gradients(zip(gradients, self.actor_critic.trainable_variables))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGdWZ4p-aNpb"
      },
      "outputs": [],
      "source": [
        "from pettingzoo.mpe import simple_speaker_listener_v3\n",
        "\n",
        "env = simple_speaker_listener_v3.env(max_cycles = 25 , continuous_actions = False)\n",
        "env.reset()\n",
        "agent_list = []\n",
        "for a in env.agents:\n",
        "  agent_list.append(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVSX5IJOaPFq"
      },
      "outputs": [],
      "source": [
        "env.reset()\n",
        "scores = []\n",
        "agent_score = {}\n",
        "hscore = -1000\n",
        "agent_high_scores = {}\n",
        "policy_net = {}\n",
        "for a in env.agents:\n",
        "  act_n = env.action_space(a).n\n",
        "  agent = Agent(alpha = 1e-5,n_actions = act_n,name = a)\n",
        "  agent_score[a] = scores\n",
        "  agent_high_scores[a] = hscore\n",
        "  policy_net[a] = agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rx_lqZkcakFj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "avg_scores= []\n",
        "n_games = 1000\n",
        "best_score = -10000\n",
        "score_history = []\n",
        "load_checkpoint = False\n",
        "prev_states = []\n",
        "for i in range(n_games):\n",
        "  env.reset()\n",
        "  score = 0\n",
        "  for agent in env.agent_iter():\n",
        "    obsv,reward,done,trunc,_ = env.last()\n",
        "    prev_states.append(obsv)\n",
        "    if not (done or trunc):\n",
        "      action = policy_net[agent].choose_action(obsv)\n",
        "      score += reward\n",
        "      env.step(action)\n",
        "      obsv_,_,_,_,_ = env.last()\n",
        "    else:\n",
        "      action = None\n",
        "      env.step(action)\n",
        "    policy_net[agent].learn(obsv,reward,prev_states[-1],done)\n",
        "    \n",
        "  score_history.append(score)\n",
        "\n",
        "  avg_score = np.mean(score_history[-100:])\n",
        "  \n",
        "  if avg_score > best_score:\n",
        "    best_score = avg_score\n",
        "    for a in agent_list:\n",
        "      policy_net[a].save_models()\n",
        "\n",
        "        \n",
        "  print('episode: ',i ,'soore: ',score,' avg_score: ', avg_score)\n",
        "  avg_scores.append(avg_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLOdLIkaCU25"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import savgol_filter\n",
        "\n",
        "x = np.linspace(0,2*np.pi,100)\n",
        "y = np.sin(x) + np.random.random(100) * 0.2\n",
        "yhat = savgol_filter(score_history, 101, 7) # window size 51, polynomial order 3\n",
        "\n",
        "\n",
        "plt.plot(yhat, color='red')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9T8FOnfxCmOS"
      },
      "outputs": [],
      "source": [
        "print((avg_scores))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyMikjDge397CBsa1csxBhqT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}