{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luthandonx/Multi-Agent-Reinforcement-Learining/blob/Simple-Adversary/MADDPG_Simple_Adversary_2_agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHe6t4r1pJHX"
      },
      "outputs": [],
      "source": [
        "!pip install pettingzoo[mpe]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQdNuZacotZ1"
      },
      "outputs": [],
      "source": [
        "from pettingzoo.mpe import simple_adversary_v2\n",
        "env = simple_adversary_v2.env(N=2,max_cycles = 25 , continuous_actions = False)\n",
        "env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVuwU7oEsU7T"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Replay():\n",
        "  def __init__(self,max_size,n_actions,input_shape):\n",
        "    self.mem_size = max_size\n",
        "    self.mem_counter = 0\n",
        "    self.state_memory = np.zeros((self.mem_size,input_shape))\n",
        "    self.new_state_memory = np.zeros((self.mem_size,input_shape))\n",
        "    self.action_memory = np.zeros((self.mem_size,n_actions))\n",
        "    self.reward_memory = np.zeros(self.mem_size)\n",
        "    self.terminal_memory = np.zeros(self.mem_size,dtype = bool)\n",
        "\n",
        "  def store_trans(self,state,action,reward,done):\n",
        "    index = self.mem_counter % self.mem_size\n",
        "    self.state_memory[index] = state\n",
        "    self.action_memory[index] = action\n",
        "    self.reward_memory[index] = reward\n",
        "   \n",
        "    self.terminal_memory[index] = done\n",
        "    if (self.mem_counter > 0):\n",
        "       self.new_state_memory[index-1] = state\n",
        "\n",
        "    self.mem_counter += 1\n",
        "\n",
        "  def sample_buffer(self,batch):\n",
        "    self.batch = batch\n",
        "    states = self.state_memory[batch]\n",
        "    actions = self.action_memory[batch]\n",
        "    rewards = self.reward_memory[batch]\n",
        "    states_ = self.new_state_memory[batch]\n",
        "    dones = self.terminal_memory[batch]\n",
        "    \n",
        "    return   states,actions,rewards,states_,dones\n",
        "\n",
        "  def returnSample(self,batch_size):\n",
        "    max_mem = min(batch_size,self.mem_counter)\n",
        "    batch = np.random.choice(max_mem,batch_size) \n",
        "    return batch\n",
        "\n",
        "  def returnshit(self):\n",
        "    return self.state_memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agL75qrmXseD"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "class CriticNet(keras.Model):\n",
        "  def __init__(self,n_actions, dim_layer):\n",
        "    super(CriticNet,self).__init__()\n",
        "    self.dim_size = dim_layer\n",
        "    self.n_actions = n_actions\n",
        "    self.dim_size = 128\n",
        "    self.layer_one = Dense(self.dim_size,activation='relu')\n",
        "    self.layer_two = Dense(self.dim_size,activation='relu')\n",
        "    self.q_value = Dense(1,activation=None)\n",
        "\n",
        "  def call(self,inputs):\n",
        "    states,actions = inputs\n",
        "    input_thing = tf.concat([states, actions],axis = 1)\n",
        "    action_value = self.layer_one(input_thing)\n",
        "    action_value = self.layer_two(action_value)\n",
        "\n",
        "    q = self.q_value(action_value)\n",
        "\n",
        "    return q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfQnMLftdaZg"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "class ActorNet(keras.Model):\n",
        "  def __init__(self,n_actions, dim_layer):\n",
        "    super(ActorNet,self).__init__()\n",
        "    self.dim_size = dim_layer\n",
        "    self.n_actions = n_actions\n",
        "\n",
        "    self.layer_one = Dense(self.dim_size, activation = 'relu')\n",
        "    self.layer_two = Dense(self.dim_size, activation = 'relu')\n",
        "    self.policy = Dense(self.n_actions, activation = 'softmax')\n",
        "\n",
        "  def call(self,state):\n",
        "    probs = self.layer_one(state)\n",
        "    probs = self.layer_two(probs)\n",
        "\n",
        "    mu = self.policy(probs)\n",
        "    return mu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent_Good:\n",
        "  def __init__(self,input_dims,alpha = 0.01,beta =1e-5 , env = None ,\n",
        "               gamma = 0.95, n_actions = 5, max_size = 10000, tau = 0.001,\n",
        "               layer_size = 256, batch_size = 50,noise = 0.1, name = None):\n",
        "    \n",
        "    self.gamma = gamma\n",
        "    self.tau = tau\n",
        "    self.memory = Replay(max_size,n_actions,input_dims)\n",
        "    self.batch_size = batch_size\n",
        "    self.n_actions = n_actions\n",
        "    self.act_name = name+' actor'\n",
        "    self.cri_name = name+' critc'\n",
        "    self.tar_act_name = name+' target_actor'\n",
        "    self.tar_cri_name = name+' target_critc'\n",
        "    \n",
        "    self.actor = ActorNet(n_actions = n_actions, dim_layer = layer_size)\n",
        "    self.critic = CriticNet(n_actions,layer_size)\n",
        "\n",
        "    self.target_actor = ActorNet(n_actions = n_actions, dim_layer = layer_size)\n",
        "    self.target_critic = CriticNet(n_actions,layer_size)\n",
        "\n",
        "    self.actor.compile(keras.optimizers.Adam(learning_rate=alpha))\n",
        "    self.target_actor.compile(keras.optimizers.Adam(learning_rate=alpha))\n",
        "\n",
        "    self.critic.compile(keras.optimizers.Adam(learning_rate=beta))\n",
        "    self.target_critic.compile(keras.optimizers.Adam(learning_rate=beta))\n",
        "\n",
        "    self.learnCounter = 0\n",
        "\n",
        "  def Update_net_parameters(self,tau=None):\n",
        "    if tau is None:\n",
        "      tau = self.tau\n",
        "    \n",
        "    weights = []\n",
        "    targets = self.target_actor.weights\n",
        "\n",
        "    for i,weight in enumerate(self.actor.weights):\n",
        "      weights.append(weight * tau + targets[i] *(1-tau))\n",
        "    self.target_actor.set_weights(weights)\n",
        "\n",
        "    weights = []\n",
        "    targets = self.target_critic.weights  \n",
        "    for i,weight in enumerate(self.critic.weights):\n",
        "      weights.append(weight * tau + targets[i] *(1-tau))\n",
        "    self.target_critic.set_weights(weights)\n",
        "\n",
        "  def store_transition(self,state,action,reward,done):\n",
        "    self.memory.store_trans(state,action,reward,done)\n",
        "\n",
        "  def save_models(self):\n",
        "    print('....saving models....')\n",
        "    self.actor.save_weights(self.act_name)\n",
        "    self.critic.save_weights(self.cri_name)\n",
        "    self.target_actor.save_weights(self.tar_act_name)\n",
        "    self.target_critic.save_weights(self.tar_cri_name)\n",
        "\n",
        "    self.actor.save(self.act_name)\n",
        "\n",
        "  def load_models(self):\n",
        "    print('....loading models....')\n",
        "    self.actor.load_weights(self.act_name)\n",
        "    self.critic.load_weights(self.cri_name)\n",
        "    self.target_actor.load_weights(self.tar_act_name)\n",
        "    self.target_critic.load_weights(self.tar_cri_name)\n",
        "\n",
        "  def choose_action(self,observation,evaluate = False):\n",
        "    state = tf.convert_to_tensor([observation])\n",
        "    probs = self.actor(state)\n",
        "    action_probs = tfp.distributions.Categorical(probs = probs)\n",
        "    action = action_probs.sample()\n",
        "    action = action.numpy()[0]\n",
        "    return action\n",
        "\n",
        "  def learn(self,agents):\n",
        "    \n",
        "    if self.memory.mem_counter < self.batch_size:\n",
        "      return\n",
        "    \n",
        "    self.learnCounter += 1\n",
        "    agent_2 = agents[0]\n",
        "\n",
        "    samples = self.memory.returnSample(self.batch_size)\n",
        "\n",
        "    state_1,action_1,reward_1,new_state_1,done_1 = self.memory.sample_buffer(samples)\n",
        "    states_1 = tf.convert_to_tensor(state_1)\n",
        "    new_states_1 = tf.convert_to_tensor(new_state_1)\n",
        "    rewards_1 = tf.convert_to_tensor(reward_1, dtype= tf.float32)\n",
        "    actions_1 = tf.convert_to_tensor(action_1,dtype= tf.float32)\n",
        "\n",
        "    state_2,action_2,reward_2,new_state_2,done_2 = agent_2.memory.sample_buffer(samples)\n",
        "    states_2 = tf.convert_to_tensor(state_2)\n",
        "    new_states_2 = tf.convert_to_tensor(new_state_2)\n",
        "    rewards_2 = tf.convert_to_tensor(reward_2)\n",
        "    actions_2 = tf.convert_to_tensor(action_2,dtype= tf.float32)\n",
        "\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        target_actions_probs_1 = self.target_actor(new_states_1)\n",
        "        target_probs = tfp.distributions.Categorical(probs = target_actions_probs_1)\n",
        "        target_actions_1 = tf.transpose([target_probs.sample()])\n",
        "    \n",
        "        target_actions_probs_2 = agent_2.target_actor(new_states_2)\n",
        "        target_probs = tfp.distributions.Categorical(probs = target_actions_probs_2)\n",
        "        target_actions_2 = tf.transpose([target_probs.sample()])\n",
        "      \n",
        "\n",
        "        actions_1 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(actions_1), [0]))\n",
        "        actions_2 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(actions_2), [0]))\n",
        "        actions = tf.concat([actions_1,actions_2],axis = 1)\n",
        "        actions = tf.cast(actions,tf.float32)\n",
        "\n",
        "        new_states = tf.concat([new_states_1,new_states_2],axis = 1)\n",
        "        target_actions = tf.concat([target_actions_1,target_actions_2], axis = 1)\n",
        "        target_actions = tf.cast(target_actions,tf.float32)\n",
        "\n",
        "      \n",
        "        critic_value_ = tf.squeeze(self.target_critic((new_states,target_actions)),1)\n",
        "        states= tf.concat([states_1,states_2],axis = 1)\n",
        "     \n",
        "        critic_value = tf.squeeze(self.critic((states,actions)),1)\n",
        "        target = rewards_1 + self.gamma*critic_value_*(1-done_1)\n",
        "        critic_loss = keras.losses.MSE(target,critic_value)\n",
        "    \n",
        "    params = self.target_critic.trainable_variables\n",
        "    grads = tape.gradient(critic_loss,params)\n",
        "    self.critic.optimizer.apply_gradients(zip(grads,params))\n",
        "\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        new_policy_actions_probs = self.actor(states_1)\n",
        "        new_probs = tfp.distributions.Categorical(probs = new_policy_actions_probs)\n",
        "        new_actions_1 = tf.transpose([new_probs.sample()])\n",
        "\n",
        "        new_actions_probs_2 = agent_2.actor(states_2)\n",
        "        new_probs = tfp.distributions.Categorical(probs = new_actions_probs_2)\n",
        "        new_actions_2 = tf.transpose([new_probs.sample()])\n",
        "\n",
        "        new_actions = tf.concat([new_actions_1,new_actions_2],axis =1 )\n",
        "        new_actions = tf.cast(new_actions,tf.float32)\n",
        "        states = tf.concat([states_1,states_2],axis = 1)\n",
        "          \n",
        "        actor_loss = -self.critic((states,new_actions))\n",
        "        actor_loss = actor_loss[0]\n",
        "    \n",
        "    params = self.critic.trainable_variables\n",
        "    grads = tape.gradient(actor_loss,params)\n",
        "    self.actor.optimizer.apply_gradients(zip(grads,params))\n",
        "\n",
        "    if (self.learnCounter % 100 == 0):\n",
        "      self.Update_net_parameters()\n",
        "      self.learnCounter = 0"
      ],
      "metadata": {
        "id": "tjADj83U9BOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seJqIe7pf8EA"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "class Agent_Opp:\n",
        "  def __init__(self,input_dims,alpha = 0.01,beta =1e-5 , env = None ,\n",
        "               gamma = 0.95, n_actions = 5, max_size = 10000, tau = 0.001,\n",
        "               layer_size = 256, batch_size = 50,noise = 0.1, name = None):\n",
        "    \n",
        "    self.gamma = gamma\n",
        "    self.tau = tau\n",
        "    self.memory = Replay(max_size,n_actions,input_dims)\n",
        "    self.batch_size = batch_size\n",
        "    self.n_actions = n_actions\n",
        "    self.act_name = name+' actor'\n",
        "    self.cri_name = name+' critc'\n",
        "    self.tar_act_name = name+' target_actor'\n",
        "    self.tar_cri_name = name+' target_critc'\n",
        "    \n",
        "    self.actor = ActorNet(n_actions = n_actions, dim_layer = layer_size)\n",
        "    self.critic = CriticNet(n_actions,layer_size)\n",
        "\n",
        "    self.target_actor = ActorNet(n_actions = n_actions, dim_layer = layer_size)\n",
        "    self.target_critic = CriticNet(n_actions,layer_size)\n",
        "\n",
        "    self.actor.compile(keras.optimizers.Adam(learning_rate=alpha))\n",
        "    self.target_actor.compile(keras.optimizers.Adam(learning_rate=alpha))\n",
        "\n",
        "    self.critic.compile(keras.optimizers.Adam(learning_rate=beta))\n",
        "    self.target_critic.compile(keras.optimizers.Adam(learning_rate=beta))\n",
        "\n",
        "    self.learnCounter = 0\n",
        "\n",
        "  def Update_net_parameters(self,tau=None):\n",
        "    if tau is None:\n",
        "      tau = self.tau\n",
        "    \n",
        "    weights = []\n",
        "    targets = self.target_actor.weights\n",
        "\n",
        "    for i,weight in enumerate(self.actor.weights):\n",
        "      weights.append(weight * tau + targets[i] *(1-tau))\n",
        "    self.target_actor.set_weights(weights)\n",
        "\n",
        "    weights = []\n",
        "    targets = self.target_critic.weights  \n",
        "    for i,weight in enumerate(self.critic.weights):\n",
        "      weights.append(weight * tau + targets[i] *(1-tau))\n",
        "    self.target_critic.set_weights(weights)\n",
        "\n",
        "  def store_transition(self,state,action,reward,done):\n",
        "    self.memory.store_trans(state,action,reward,done)\n",
        "\n",
        "  def save_models(self):\n",
        "    print('....saving models....')\n",
        "    self.actor.save_weights(self.act_name)\n",
        "    self.critic.save_weights(self.cri_name)\n",
        "    self.target_actor.save_weights(self.tar_act_name)\n",
        "    self.target_critic.save_weights(self.tar_cri_name)\n",
        "\n",
        "    self.actor.save(self.act_name)\n",
        "\n",
        "  def load_models(self):\n",
        "    print('....loading models....')\n",
        "    self.actor.load_weights(self.act_name)\n",
        "    self.critic.load_weights(self.cri_name)\n",
        "    self.target_actor.load_weights(self.tar_act_name)\n",
        "    self.target_critic.load_weights(self.tar_cri_name)\n",
        "\n",
        "  def choose_action(self,observation,evaluate = False):\n",
        "    state = tf.convert_to_tensor([observation])\n",
        "    probs = self.actor(state)\n",
        "    action_probs = tfp.distributions.Categorical(probs = probs)\n",
        "    action = action_probs.sample()\n",
        "    action = action.numpy()[0]\n",
        "    return action\n",
        "\n",
        "  def learn(self):\n",
        "    if self.memory.mem_counter < self.batch_size:\n",
        "      return\n",
        "\n",
        "    self.learnCounter += 1\n",
        "\n",
        "    samples = self.memory.returnSample(self.batch_size)\n",
        "\n",
        "    state_1,action_1,reward_1,new_state_1,done_1 = self.memory.sample_buffer(samples)\n",
        "    states_1 = tf.convert_to_tensor(state_1)\n",
        "    new_states_1 = tf.convert_to_tensor(new_state_1)\n",
        "    rewards_1 = tf.convert_to_tensor(reward_1, dtype= tf.float32)\n",
        "    actions_1 = tf.convert_to_tensor(action_1,dtype= tf.float32)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        target_actions_probs_1 = self.target_actor(new_states_1)\n",
        "        target_probs = tfp.distributions.Categorical(probs = target_actions_probs_1)\n",
        "        target_actions_1 = tf.transpose([target_probs.sample()])  \n",
        "\n",
        "        actions_1 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(actions_1), [0]))\n",
        "        actions = tf.cast(actions_1,tf.float32)\n",
        "\n",
        "        target_actions = tf.cast(target_actions_1,tf.float32)\n",
        "        critic_value_ = tf.squeeze(self.target_critic((new_states_1,target_actions)),1)\n",
        "        states = states_1\n",
        "        critic_value = tf.squeeze(self.critic((states,actions)),1)\n",
        "        target = rewards_1 + self.gamma*critic_value_*(1-done_1)\n",
        "        critic_loss = keras.losses.MSE(target,critic_value)\n",
        "    params = self.target_critic.trainable_variables\n",
        "    grads = tape.gradient(critic_loss,params)\n",
        "    self.critic.optimizer.apply_gradients(zip(grads,params))\n",
        "\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        new_policy_actions_probs = self.actor(states_1)\n",
        "        new_probs = tfp.distributions.Categorical(probs = new_policy_actions_probs)\n",
        "        new_actions_1 = tf.transpose([new_probs.sample()])\n",
        "        new_actions = tf.cast(new_actions_1,tf.float32)\n",
        "        states = states_1\n",
        "        actor_loss = -self.critic((states,new_actions))\n",
        "        actor_loss = actor_loss[0]\n",
        "    params = self.critic.trainable_variables\n",
        "    grads = tape.gradient(actor_loss,params)\n",
        "    self.actor.optimizer.apply_gradients(zip(grads,params))\n",
        "\n",
        "    if (self.learnCounter % 100 == 0):\n",
        "      self.Update_net_parameters()\n",
        "      self.learnCounter = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mfQlx_jpTuZ"
      },
      "outputs": [],
      "source": [
        "env.reset()\n",
        "agent_net = {}\n",
        "agent_list = []\n",
        "for a in env.agents:\n",
        "  n_actions = env.action_space(a).n\n",
        "  input_dims = env.observation_space(a).shape[0]\n",
        "  if(a == 'adversary_0'):\n",
        "    agent_Opp = Agent_Opp(input_dims= input_dims ,alpha = 1e-5,env = env , n_actions = n_actions,name = a)\n",
        "  else:\n",
        "    agent =  Agent_Good(input_dims= input_dims ,alpha = 1e-5,env = env , n_actions = n_actions,name = a)\n",
        "    agent_list.append(a)\n",
        "    agent_net[a] = agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_VeWxC5qyhr"
      },
      "outputs": [],
      "source": [
        "n_games = 2500\n",
        "Good_best_score = -1000\n",
        "Opp_best_score = -1000\n",
        "Opp_score_history = []\n",
        "Good_score_history = []\n",
        "Opp_state_ = []\n",
        "Good_state_ = []\n",
        "load_checkpoint = False\n",
        "if load_checkpoint:\n",
        "  for a in agent_list:\n",
        "    agent_net[a].load_models()\n",
        "else:\n",
        "  evaluate = False\n",
        "for i in range(n_games):\n",
        "  Opp_score = 0\n",
        "  Good_score = 0\n",
        "  env.reset(seed = 3)\n",
        "  for agent in env.agent_iter():\n",
        "    state,reward,done,trunc,info = env.last()\n",
        "    if (agent == 'adversary_0'):\n",
        "      if not (done or trunc):\n",
        "        action = agent_Opp.choose_action(state,evaluate)\n",
        "        env.step(action)\n",
        "        Opp_score += reward\n",
        "        agent_Opp.store_transition(state,action,reward,done)\n",
        "        agent_Opp.learn() \n",
        "      else:\n",
        "        env.step(None)\n",
        "    else:\n",
        "      if not (done or trunc):\n",
        "        action = agent_net[agent].choose_action(state,evaluate)\n",
        "        env.step(action)\n",
        "        Good_score += reward\n",
        "        for a in agent_list:\n",
        "          if (a != agent):  \n",
        "            agent_net[agent].learn([agent_net[a]])\n",
        "      else:\n",
        "        env.step(None)\n",
        "\n",
        "  Good_score_history.append(Good_score)\n",
        "  Opp_score_history.append(Opp_score)\n",
        "  Good_avg_score = np.mean(Good_score_history[-100:])\n",
        "  Opp_avg_score = np.mean(Opp_score_history[-100:])\n",
        "\n",
        "  if Good_avg_score > Good_best_score:\n",
        "    Good_best_score = Good_avg_score\n",
        "    if not load_checkpoint:\n",
        "      for a in agent_list:\n",
        "        agent_net[a].save_models()\n",
        "\n",
        "  if Opp_avg_score > Opp_best_score:\n",
        "    Opp_best_score = Opp_avg_score\n",
        "    if not load_checkpoint:\n",
        "      agent_Opp.save_models()\n",
        "  \n",
        "  \n",
        "  print('episode: ', i,'Opp average score: %.2f' % Opp_avg_score,'Good average score %.2f' % Good_avg_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2368h6WqW34X"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import savgol_filter\n",
        "\n",
        "x = np.linspace(0,2*np.pi,100)\n",
        "y = np.sin(x) + np.random.random(100) * 0.2\n",
        "yhat = savgol_filter(Good_score_history, 101, 7) # window size 51, polynomial order 3\n",
        "\n",
        "\n",
        "plt.plot(yhat, color='red')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import savgol_filter\n",
        "\n",
        "x = np.linspace(0,2*np.pi,100)\n",
        "y = np.sin(x) + np.random.random(100) * 0.2\n",
        "yhat = savgol_filter(Opp_score_history, 101, 7) # window size 51, polynomial order 3\n",
        "\n",
        "\n",
        "plt.plot(yhat, color='red')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6TCsxuotEQ3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSGoC__asq3W"
      },
      "outputs": [],
      "source": [
        "print(Good_score_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKeiqFVIPQ3c"
      },
      "outputs": [],
      "source": [
        "print(Opp_score_history)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/adversary_0_actor.zip /content/adversary_0_actor\n",
        "!zip -r /content/agent_0_actor.zip /content/agent_0_actor\n",
        "!zip -r /content/agent_1_actor.zip /content/agent_1_actor"
      ],
      "metadata": {
        "id": "shF3tpIexyuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('/content/agent_1_actor.zip')\n",
        "files.download('/content/agent_0_actor.zip')\n",
        "files.download('/content/adversary_0_actor.zip')"
      ],
      "metadata": {
        "id": "Jzp8ITeIyeCC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}