{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVfCzyTuP-k1",
        "outputId": "7e82e6ca-1cc4-4105-a43f-9df199975179"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pettingzoo[mpe]\n",
            "  Downloading PettingZoo-1.22.2-py3-none-any.whl (816 kB)\n",
            "\u001b[K     |████████████████████████████████| 816 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting gymnasium>=0.26.0\n",
            "  Downloading Gymnasium-0.26.3-py3-none-any.whl (836 kB)\n",
            "\u001b[K     |████████████████████████████████| 836 kB 44.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from pettingzoo[mpe]) (1.21.6)\n",
            "Collecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.3 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gymnasium>=0.26.0->pettingzoo[mpe]) (4.13.0)\n",
            "Collecting gymnasium-notices>=0.0.1\n",
            "  Downloading gymnasium_notices-0.0.1-py3-none-any.whl (2.8 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gymnasium>=0.26.0->pettingzoo[mpe]) (1.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gymnasium>=0.26.0->pettingzoo[mpe]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gymnasium>=0.26.0->pettingzoo[mpe]) (3.10.0)\n",
            "Installing collected packages: gymnasium-notices, gymnasium, pygame, pettingzoo\n",
            "Successfully installed gymnasium-0.26.3 gymnasium-notices-0.0.1 pettingzoo-1.22.2 pygame-2.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pettingzoo[mpe]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input,Dense,Activation\n",
        "from keras.models import Model,load_model\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import keras.backend as K\n",
        "\n",
        "class Agent(object):\n",
        "  def __init__ (self,name,alpha,gamma,input_dims, n_actions,layer_size,fname ='reinforcePolicy'):\n",
        "    self.gamma = gamma\n",
        "    self.lr = alpha\n",
        "    self.G = 0\n",
        "    self.input_dims = input_dims\n",
        "    self.layer_size = layer_size\n",
        "    self.n_actions = n_actions\n",
        "    self.state_memory = []\n",
        "    self.action_memory = []\n",
        "    self.reward_memory = []\n",
        "    self.agent_name = name\n",
        "\n",
        "    self.policy,self.predict = self.create_policy()\n",
        "    self.action_space = [i for i in range(n_actions)]\n",
        "    self.model_file = fname +'_name'\n",
        "\n",
        "  def create_policy(self):\n",
        "    input = Input(shape = (self.input_dims,))#comma indicates that it takes a batch\n",
        "    advantages = Input(shape =[1])\n",
        "    dense1 = Dense(self.layer_size,activation = 'relu')(input)\n",
        "    dense2 = Dense(self.layer_size,activation = 'relu')(dense1)\n",
        "    probs = Dense(self.n_actions, activation = 'softmax')(dense2)\n",
        "\n",
        "    def custom_loss(y_true,y_pred):\n",
        "      out = K.clip(y_pred,1e-8, 1-1e-8) #this is to ensure that we do not perform log calcualtions with log values of 0\n",
        "      log_lik = y_true*K.log(out)\n",
        "\n",
        "      return K.sum(-log_lik*advantages)\n",
        "    \n",
        "    policy = Model(inputs = [input,advantages], outputs = [probs])\n",
        "    opt = keras.optimizers.Adam(learning_rate = self.lr)\n",
        "    policy.compile(optimizer = opt, loss = custom_loss)\n",
        "\n",
        "    predict = keras.Model(inputs = [input], outputs = [probs])\n",
        "\n",
        "    return policy,predict\n",
        "\n",
        "  def choose_action(self,obsv): \n",
        "    obsv = np.expand_dims(obsv,0)\n",
        "    #to keep the input shape consistant\n",
        "    probabilities = self.predict.predict(obsv)[0]\n",
        "    # we take the 0th element because predict returns a tuple\n",
        "    action = np.random.choice(self.action_space,p=probabilities)\n",
        "    # action = np.argmax(probabilities)\n",
        "    return action\n",
        "\n",
        "  def store_trans(self,obsv,action,reward):\n",
        "    self.action_memory.append(action)\n",
        "    self.state_memory.append(obsv)\n",
        "    self.reward_memory.append(reward)\n",
        "\n",
        "  def learn(self):\n",
        "    state_memory = np.array(self.state_memory)\n",
        "    reward_memory = np.array(self.reward_memory)\n",
        "    action_memory = np.array(self.action_memory)\n",
        "\n",
        "\n",
        "    actions = np.zeros([len(action_memory),self.n_actions])\n",
        "    actions[np.arange(len(action_memory)),action_memory] = 1\n",
        "    # loss function requires the labels to be 1 hot encoded\n",
        "\n",
        "    G = np.zeros_like(reward_memory)\n",
        "    for t in range(len(reward_memory)):\n",
        "      G_sum = 0\n",
        "      discount = 1\n",
        "      for k in range(t,len(reward_memory)):\n",
        "        G_sum += reward_memory[k]*discount\n",
        "        discount *= self.gamma\n",
        "      G[t] = G_sum\n",
        "\n",
        "    mean = np.mean(G)\n",
        "    std = np.std(G) if np.std(G) > 0 else 1\n",
        "    self.G = (G-mean)/std\n",
        "    print(len(self.G))\n",
        "    print(len(state_memory))\n",
        "    print(len(actions))\n",
        "    cost = self.policy.train_on_batch([state_memory,self.G], actions) # actions is our labels\n",
        "    #[state_memory,self.G] =  y_pred and actions = y_true\n",
        "    # This is to calculate for the loss function\n",
        "\n",
        "    self.state_memory = []\n",
        "    self.reward_memory = []\n",
        "    self.action_memory = []\n",
        "\n",
        "  def save_model(self):\n",
        "    self.policy.save(self.model_file)\n",
        "\n",
        "  def load_model(self):\n",
        "    self.policy = keras.models.load_model(self.model_file)"
      ],
      "metadata": {
        "id": "bf8gyKStQa78"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "\n",
        "disable_eager_execution()"
      ],
      "metadata": {
        "id": "aR7ti1P5Qo60"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pettingzoo.mpe import simple_speaker_listener_v3\n",
        "\n",
        "env = simple_speaker_listener_v3.env(max_cycles = 25 , continuous_actions = False)\n",
        "env.reset()"
      ],
      "metadata": {
        "id": "3G31wF0OQqrk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for a in env.agents:\n",
        "  x = env.action_space(a).n\n",
        "  print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-w_h3hNRKkO",
        "outputId": "d74f39f9-7b46-4c12-f0de-7406fdf9bddc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_net = {}\n",
        "agent_list = []\n",
        "for a in env.agents:\n",
        "  obs_space = env.observation_space(a).shape\n",
        "  print(obs_space[0])\n",
        "  action_space = env.action_space(a).n\n",
        "  agent = Agent(a, 1e-5 , 0.95 , obs_space[0] ,action_space,256)\n",
        "  agent_net[a] = agent\n",
        "  agent_list.append(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hj0n-oJOQsUc",
        "outputId": "9559da56-ff49-49ce-f219-8b35b334a179"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_list = env.agents\n",
        "print(agent_list)"
      ],
      "metadata": {
        "id": "yMUeVM9GPRU0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "585fe63b-704d-4f55-9e12-4e0b631bf9f0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['speaker_0', 'listener_0']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()\n",
        "best_score = -1000\n",
        "max_episode = 2000\n",
        "score_history = []\n",
        "avg_scores = []\n",
        "for i in range(max_episode):\n",
        "  score = 0\n",
        "  env.reset()\n",
        "  for agent in env.agent_iter():\n",
        "    observation,reward,done,trunc,_= env.last()\n",
        "    score += reward\n",
        "    if not (done or trunc):\n",
        "      action = agent_net[agent].choose_action(observation)\n",
        "      env.step(action)\n",
        "      agent_net[agent].store_trans(observation,action,reward)\n",
        "    else:\n",
        "      action = None\n",
        "      env.step(action)\n",
        "  \n",
        "  score_history.append(score)    \n",
        "  avg_score = np.mean(score_history[-100:])\n",
        "\n",
        "  if avg_score > best_score:\n",
        "    load_checkpoint = True\n",
        "    best_score = avg_score\n",
        "    for a in agent_list:\n",
        "      agent_net[a].save_model()\n",
        "\n",
        "  for agent in env.agents:\n",
        "    agent_net[agent].learn()\n",
        "  \n",
        "  print('episode: ', i , ' score :',score,' avg score:', np.mean(score_history[-100:]))\n",
        "  avg_scores.append(avg_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "voGBZXngQuOk",
        "outputId": "0f3d694c-1852-48eb-f516-8222f0399329"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode:  0  score : -18.70802940213866  avg score: -18.70802940213866\n",
            "episode:  1  score : -94.59987048424445  avg score: -56.653949943191556\n",
            "episode:  2  score : -343.72645558341674  avg score: -152.34478515659995\n",
            "episode:  3  score : -63.78714101641408  avg score: -130.2053741215535\n",
            "episode:  4  score : -14.220391798517333  avg score: -107.00837765694625\n",
            "episode:  5  score : -90.32046334238781  avg score: -104.22705860451985\n",
            "episode:  6  score : -37.089190701012626  avg score: -94.63593461830453\n",
            "episode:  7  score : -22.578769944684105  avg score: -85.62878903410197\n",
            "episode:  8  score : -33.16131308549883  avg score: -79.79906948425717\n",
            "episode:  9  score : -37.49757278857258  avg score: -75.56891981468871\n",
            "episode:  10  score : -62.37313150748031  avg score: -74.36930269585159\n",
            "episode:  11  score : -3.833253274976945  avg score: -68.49129857744536\n",
            "episode:  12  score : -209.1742502687352  avg score: -79.31306409215996\n",
            "episode:  13  score : -37.31175204209558  avg score: -76.31297037429822\n",
            "episode:  14  score : -53.25864923127366  avg score: -74.77601563142993\n",
            "episode:  15  score : -128.02604168891455  avg score: -78.10414226002271\n",
            "episode:  16  score : -46.41154523090122  avg score: -76.23987184654499\n",
            "episode:  17  score : -89.7373319560366  avg score: -76.98973074151674\n",
            "episode:  18  score : -74.52090427108419  avg score: -76.85979250623082\n",
            "episode:  19  score : -9.09112482793751  avg score: -73.47135912231616\n",
            "episode:  20  score : -10.308379653893686  avg score: -70.46359819524842\n",
            "episode:  21  score : -168.15288081739072  avg score: -74.90402013261853\n",
            "episode:  22  score : -19.892468532745834  avg score: -72.51221354131971\n",
            "episode:  23  score : -85.34013219740513  avg score: -73.04671015198994\n",
            "episode:  24  score : -110.57357273276088  avg score: -74.54778465522078\n",
            "episode:  25  score : -2.5301982553598408  avg score: -71.77787748599536\n",
            "episode:  26  score : -260.643306914019  avg score: -78.77289339073697\n",
            "episode:  27  score : -63.27520408616402  avg score: -78.21940448700222\n",
            "episode:  28  score : -148.12352669982198  avg score: -80.62989145985809\n",
            "episode:  29  score : -5.691156397806739  avg score: -78.13193362445638\n",
            "episode:  30  score : -39.14928856135274  avg score: -76.87442894500143\n",
            "episode:  31  score : -29.549239145098515  avg score: -75.39551676375444\n",
            "episode:  32  score : -22.42106482646311  avg score: -73.79023034141228\n",
            "episode:  33  score : -46.077190207694  avg score: -72.97514092571468\n",
            "episode:  34  score : -105.35932356963042  avg score: -73.90040328696941\n",
            "episode:  35  score : -10.148007084280247  avg score: -72.12950339245027\n",
            "episode:  36  score : -70.99926091376074  avg score: -72.09895629843163\n",
            "episode:  37  score : -30.475722054795558  avg score: -71.00360802886226\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-53f814a7912e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m       \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent_net\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m       \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0magent_net\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_trans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-4d1743af9bff>\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(self, obsv)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobsv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# we take the 0th element because predict returns a tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;31m# action = np.argmax(probabilities)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import savgol_filter\n",
        "\n",
        "x = np.linspace(0,2*np.pi,100)\n",
        "y = np.sin(x) + np.random.random(100) * 0.2\n",
        "yhat = savgol_filter(score_history, 101, 3) # window size 51, polynomial order 3\n",
        "\n",
        "\n",
        "plt.plot(yhat, color='red')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tmyd5GOHGvqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(avg_scores)"
      ],
      "metadata": {
        "id": "gbsEpaIV5xaT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}