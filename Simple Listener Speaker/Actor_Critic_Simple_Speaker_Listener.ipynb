{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leZ2Jd3yZygb",
        "outputId": "671d205e-8bcd-4940-b32a-faf4f78e06eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pettingzoo[mpe]\n",
            "  Downloading PettingZoo-1.22.2-py3-none-any.whl (816 kB)\n",
            "\u001b[K     |████████████████████████████████| 816 kB 6.3 MB/s \n",
            "\u001b[?25hCollecting gymnasium>=0.26.0\n",
            "  Downloading Gymnasium-0.26.3-py3-none-any.whl (836 kB)\n",
            "\u001b[K     |████████████████████████████████| 836 kB 50.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from pettingzoo[mpe]) (1.21.6)\n",
            "Collecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.3 MB 93 kB/s \n",
            "\u001b[?25hCollecting gymnasium-notices>=0.0.1\n",
            "  Downloading gymnasium_notices-0.0.1-py3-none-any.whl (2.8 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gymnasium>=0.26.0->pettingzoo[mpe]) (1.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gymnasium>=0.26.0->pettingzoo[mpe]) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gymnasium>=0.26.0->pettingzoo[mpe]) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gymnasium>=0.26.0->pettingzoo[mpe]) (4.1.1)\n",
            "Installing collected packages: gymnasium-notices, gymnasium, pygame, pettingzoo\n",
            "Successfully installed gymnasium-0.26.3 gymnasium-notices-0.0.1 pettingzoo-1.22.2 pygame-2.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pettingzoo[mpe]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bbEIoqgmZ_4D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "class ActorCriticNetwork(keras.Model):\n",
        "  def __init__(self,n_actions, dims_1 = 256 , dims_2 = 256,\n",
        "               name = 'actor_critc', chkpt_dir = 'tmp/actor_critic'):\n",
        "    super(ActorCriticNetwork, self).__init__()\n",
        "    self.dims_1 = dims_1\n",
        "    self.dims_2 = dims_2\n",
        "    self.n_actions = n_actions\n",
        "    self.model_name = name\n",
        "    self.checkpoint_dir = chkpt_dir\n",
        "    self.checkpoint_file = os.path.join(self.checkpoint_dir, name+'_ac')\n",
        "\n",
        "    self.fc1 = Dense(self.dims_1,activation = 'relu')\n",
        "    self.fc2 = Dense(self.dims_2,activation = 'relu')\n",
        "    self.v = Dense(1,activation = None)\n",
        "    self.po = Dense(n_actions,activation = 'softmax')\n",
        "\n",
        "  def call(self,state):\n",
        "    value = self.fc1(state)\n",
        "    value = self.fc2(value)\n",
        "\n",
        "    v = self.v(value)\n",
        "    po = self.po(value)\n",
        "\n",
        "    return v,po"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PaKCMHkNaFCa"
      },
      "outputs": [],
      "source": [
        "from re import A\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "\n",
        "class Agent:\n",
        "  def __init__(self, alpha = 1e-5, gamma = 0.95, n_actions = 4,name='agent'):\n",
        "    self.gamma = gamma\n",
        "    self.n_actions = n_actions\n",
        "    self.action = None\n",
        "    self.action_space = [i for i in range(self.n_actions)]\n",
        "    self.name = name\n",
        "    self.actor_critic = ActorCriticNetwork(n_actions=n_actions)\n",
        "    self.actor_critic.compile(optimizer = Adam(learning_rate = alpha))\n",
        "    self.fname = 'actor_critic_'+self.name\n",
        "\n",
        "   \n",
        "    self.reward_history = []\n",
        "    self.action_history = []\n",
        "    self.obsv_history = []\n",
        "    self.next_obsv_history = []\n",
        "\n",
        "  def choose_action(self,observation):\n",
        "    state = tf.convert_to_tensor([observation])\n",
        "    _,probs = self.actor_critic(state)\n",
        "\n",
        "    action_probs = tfp.distributions.Categorical(probs = probs)\n",
        "    action = action_probs.sample()\n",
        "\n",
        "    self.action = action\n",
        "\n",
        "    return action.numpy()[0]\n",
        "\n",
        "  def save_models(self):\n",
        "    print('saving model.....')\n",
        "    self.actor_critic.save(self.fname)\n",
        "  \n",
        "  def load_models(self):\n",
        "    #print('loading model....')\n",
        "    self.actor_critic = keras.models.load_model(self.fname)\n",
        "\n",
        "  def store_trans(self,action,reward,obsv,obsv_):\n",
        "      self.action_history.append(action)\n",
        "      self.reward_history.append(reward)\n",
        "      self.obsv_history.append(obsv)\n",
        "      self.next_obsv_history.append(obsv_)\n",
        "\n",
        "  def clear_memory(self):\n",
        "    self.reward_history = []\n",
        "    self.action_history = []\n",
        "    self.obsv_history = []\n",
        "    self.next_obsv_history = []\n",
        "\n",
        "  def learn(self,state,reward,state_,done):\n",
        "    state = tf.convert_to_tensor([state],dtype = tf.float32)\n",
        "    state_ = tf.convert_to_tensor([state_],dtype = tf.float32)\n",
        "    reward = tf.convert_to_tensor(reward,dtype = tf.float32)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      state_value, probs = self.actor_critic(state)\n",
        "      state_value_,_ = self.actor_critic(state_)\n",
        "      state_value = tf.squeeze(state_value)\n",
        "      state_value_ = tf.squeeze(state_value_)\n",
        "\n",
        "      action_probs = tfp.distributions.Categorical(probs = probs)\n",
        "      log_probs = action_probs.log_prob(self.action)\n",
        "\n",
        "      delta = reward + self.gamma*state_value_*(1-int(done)) - state_value\n",
        "\n",
        "      actor_loss = -log_probs*delta\n",
        "      critic_loss = delta**2\n",
        "\n",
        "      total_loss = actor_loss + critic_loss\n",
        "\n",
        "    gradients = tape.gradient(total_loss,self.actor_critic.trainable_variables)\n",
        "    self.actor_critic.optimizer.apply_gradients(zip(gradients, self.actor_critic.trainable_variables))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VGdWZ4p-aNpb"
      },
      "outputs": [],
      "source": [
        "from pettingzoo.mpe import simple_speaker_listener_v3\n",
        "\n",
        "env = simple_speaker_listener_v3.env(max_cycles = 25 , continuous_actions = False)\n",
        "env.reset()\n",
        "agent_list = []\n",
        "for a in env.agents:\n",
        "  agent_list.append(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VVSX5IJOaPFq"
      },
      "outputs": [],
      "source": [
        "env.reset()\n",
        "scores = []\n",
        "agent_score = {}\n",
        "hscore = -1000\n",
        "agent_high_scores = {}\n",
        "policy_net = {}\n",
        "for a in env.agents:\n",
        "  act_n = env.action_space(a).n\n",
        "  agent = Agent(alpha = 1e-5,n_actions = act_n,name = a)\n",
        "  agent_score[a] = scores\n",
        "  agent_high_scores[a] = hscore\n",
        "  policy_net[a] = agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "rx_lqZkcakFj",
        "outputId": "c0a7479a-a0fa-4b77-ea7b-c0dd5508e648"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving model.....\n",
            "saving model.....\n",
            "episode:  0 soore:  -91.00413583100885  avg_score:  -91.00413583100885\n",
            "saving model.....\n",
            "saving model.....\n",
            "episode:  1 soore:  -13.136513809246434  avg_score:  -52.070324820127645\n",
            "episode:  2 soore:  -119.86820980817767  avg_score:  -74.66961981614432\n",
            "episode:  3 soore:  -183.58894637078802  avg_score:  -101.89945145480525\n",
            "episode:  4 soore:  -141.6664039377901  avg_score:  -109.85284195140221\n",
            "episode:  5 soore:  -68.17194776948688  avg_score:  -102.90602625441632\n",
            "episode:  6 soore:  -146.6885247684609  avg_score:  -109.16066889927983\n",
            "episode:  7 soore:  -126.46145934777263  avg_score:  -111.32326770534144\n",
            "episode:  8 soore:  -95.66017536095521  avg_score:  -109.58292411152075\n",
            "episode:  9 soore:  -1.529872958779796  avg_score:  -98.77761899624666\n",
            "episode:  10 soore:  -40.930144855347066  avg_score:  -93.51875771071032\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "avg_scores= []\n",
        "n_games = 1000\n",
        "best_score = -10000\n",
        "score_history = []\n",
        "load_checkpoint = False\n",
        "prev_states = []\n",
        "for i in range(n_games):\n",
        "  env.reset()\n",
        "  score = 0\n",
        "  for agent in env.agent_iter():\n",
        "    obsv,reward,done,trunc,_ = env.last()\n",
        "    prev_states.append(obsv)\n",
        "    if not (done or trunc):\n",
        "      action = policy_net[agent].choose_action(obsv)\n",
        "      score += reward\n",
        "      env.step(action)\n",
        "      obsv_,_,_,_,_ = env.last()\n",
        "    else:\n",
        "      action = None\n",
        "      env.step(action)\n",
        "    policy_net[agent].learn(obsv,reward,prev_states[-1],done)\n",
        "    \n",
        "  score_history.append(score)\n",
        "\n",
        "  avg_score = np.mean(score_history[-100:])\n",
        "  \n",
        "  if avg_score > best_score:\n",
        "    best_score = avg_score\n",
        "    for a in agent_list:\n",
        "      policy_net[a].save_models()\n",
        "\n",
        "        \n",
        "  print('episode: ',i ,'soore: ',score,' avg_score: ', avg_score)\n",
        "  avg_scores.append(avg_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vLOdLIkaCU25"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import savgol_filter\n",
        "\n",
        "x = np.linspace(0,2*np.pi,100)\n",
        "y = np.sin(x) + np.random.random(100) * 0.2\n",
        "yhat = savgol_filter(score_history, 101, 7) # window size 51, polynomial order 3\n",
        "\n",
        "\n",
        "plt.plot(yhat, color='red')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9T8FOnfxCmOS"
      },
      "outputs": [],
      "source": [
        "print((avg_scores))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}