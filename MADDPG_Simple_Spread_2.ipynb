{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luthandonx/Multi-Agent-Reinforcement-Learining/blob/Simple-Spread/MADDPG_Simple_Spread_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHe6t4r1pJHX",
        "outputId": "6eb4acfe-20d7-46f2-fe99-0f0932f2756d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pettingzoo[mpe]\n",
            "  Downloading PettingZoo-1.22.2-py3-none-any.whl (816 kB)\n",
            "\u001b[K     |████████████████████████████████| 816 kB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from pettingzoo[mpe]) (1.21.6)\n",
            "Collecting gymnasium>=0.26.0\n",
            "  Downloading Gymnasium-0.26.3-py3-none-any.whl (836 kB)\n",
            "\u001b[K     |████████████████████████████████| 836 kB 53.7 MB/s \n",
            "\u001b[?25hCollecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.3 MB 89 kB/s \n",
            "\u001b[?25hCollecting gymnasium-notices>=0.0.1\n",
            "  Downloading gymnasium_notices-0.0.1-py3-none-any.whl (2.8 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gymnasium>=0.26.0->pettingzoo[mpe]) (4.13.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gymnasium>=0.26.0->pettingzoo[mpe]) (1.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gymnasium>=0.26.0->pettingzoo[mpe]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gymnasium>=0.26.0->pettingzoo[mpe]) (3.10.0)\n",
            "Installing collected packages: gymnasium-notices, gymnasium, pygame, pettingzoo\n",
            "Successfully installed gymnasium-0.26.3 gymnasium-notices-0.0.1 pettingzoo-1.22.2 pygame-2.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pettingzoo[mpe]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQdNuZacotZ1"
      },
      "outputs": [],
      "source": [
        "from pettingzoo.mpe import simple_spread_v2\n",
        "\n",
        "env = simple_spread_v2.env(N=2, local_ratio=0.5, max_cycles=25, continuous_actions=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVuwU7oEsU7T"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Replay():\n",
        "  def __init__(self,max_size,n_actions,input_shape):\n",
        "    self.mem_size = max_size\n",
        "    self.mem_counter = 0\n",
        "    self.state_memory = np.zeros((self.mem_size,input_shape))\n",
        "    self.new_state_memory = np.zeros((self.mem_size,input_shape))\n",
        "    self.action_memory = np.zeros((self.mem_size,n_actions))\n",
        "    self.reward_memory = np.zeros(self.mem_size)\n",
        "    self.terminal_memory = np.zeros(self.mem_size,dtype = bool)\n",
        "\n",
        "  def store_trans(self,state,action,reward,state_,done):\n",
        "    index = self.mem_counter % self.mem_size\n",
        "    self.state_memory[index] = state\n",
        "    self.action_memory[index] = action\n",
        "    self.reward_memory[index] = reward\n",
        "    self.new_state_memory[index] = state_\n",
        "    self.terminal_memory[index] = done\n",
        "\n",
        "    self.mem_counter += 1\n",
        "\n",
        "  def sample_buffer(self,batch):\n",
        "    self.batch = batch\n",
        "    states = self.state_memory[batch]\n",
        "    actions = self.action_memory[batch]\n",
        "    rewards = self.reward_memory[batch]\n",
        "    states_ = self.new_state_memory[batch]\n",
        "    dones = self.terminal_memory[batch]\n",
        "    \n",
        "    return   states,actions,rewards,states_,dones\n",
        "\n",
        "  def returnSample(self,batch_size):\n",
        "    max_mem = min(batch_size,self.mem_counter)\n",
        "    batch = np.random.choice(max_mem,batch_size) \n",
        "    return batch\n",
        "\n",
        "  def returnshit(self):\n",
        "    return self.state_memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agL75qrmXseD"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "class CriticNet(keras.Model):\n",
        "  def __init__(self,n_actions, dim_layer):\n",
        "    super(CriticNet,self).__init__()\n",
        "    self.dim_size = dim_layer\n",
        "    self.n_actions = n_actions\n",
        "    self.dim_size = 128\n",
        "    self.layer_one = Dense(self.dim_size,activation='relu')\n",
        "    self.layer_two = Dense(self.dim_size,activation='relu')\n",
        "    self.q_value = Dense(1,activation=None)\n",
        "\n",
        "  def call(self,inputs):\n",
        "    states,actions = inputs\n",
        "    input_thing = tf.concat([states, actions],axis = 1)\n",
        "    action_value = self.layer_one(input_thing)\n",
        "    action_value = self.layer_two(action_value)\n",
        "\n",
        "    q = self.q_value(action_value)\n",
        "\n",
        "    return q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfQnMLftdaZg"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "class ActorNet(keras.Model):\n",
        "  def __init__(self,n_actions, dim_layer):\n",
        "    super(ActorNet,self).__init__()\n",
        "    self.dim_size = dim_layer\n",
        "    self.n_actions = n_actions\n",
        "\n",
        "    self.layer_one = Dense(self.dim_size, activation = 'relu')\n",
        "    self.layer_two = Dense(self.dim_size, activation = 'relu')\n",
        "    self.policy = Dense(self.n_actions, activation = 'softmax')\n",
        "\n",
        "  def call(self,state):\n",
        "    probs = self.layer_one(state)\n",
        "    probs = self.layer_two(probs)\n",
        "\n",
        "    mu = self.policy(probs)\n",
        "    return mu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seJqIe7pf8EA"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "class Agent:\n",
        "  def __init__(self,input_dims,alpha = 0.01,beta =1e-5 , env = None ,\n",
        "               gamma = 0.95, n_actions = 5, max_size = 10000, tau = 0.001,\n",
        "               layer_size = 256, batch_size = 50,noise = 0.1, name = None):\n",
        "    \n",
        "    self.gamma = gamma\n",
        "    self.tau = tau\n",
        "    self.memory = Replay(max_size,n_actions,input_dims)\n",
        "    self.batch_size = batch_size\n",
        "    self.n_actions = n_actions\n",
        "    self.act_name = name+' actor'\n",
        "    self.cri_name = name+' critc'\n",
        "    self.tar_act_name = name+' target_actor'\n",
        "    self.tar_cri_name = name+' target_critc'\n",
        "    \n",
        "    self.actor = ActorNet(n_actions = n_actions, dim_layer = layer_size)\n",
        "    self.critic = CriticNet(n_actions,layer_size)\n",
        "\n",
        "    self.target_actor = ActorNet(n_actions = n_actions, dim_layer = layer_size)\n",
        "    self.target_critic = CriticNet(n_actions,layer_size)\n",
        "\n",
        "    self.actor.compile(keras.optimizers.Adam(learning_rate=alpha))\n",
        "    self.target_actor.compile(keras.optimizers.Adam(learning_rate=alpha))\n",
        "\n",
        "    self.critic.compile(keras.optimizers.Adam(learning_rate=beta))\n",
        "    self.target_critic.compile(keras.optimizers.Adam(learning_rate=beta))\n",
        "\n",
        "    self.learnCounter = 0\n",
        "\n",
        "  def Update_net_parameters(self,tau=None):\n",
        "    if tau is None:\n",
        "      tau = self.tau\n",
        "    \n",
        "    weights = []\n",
        "    targets = self.target_actor.weights\n",
        "\n",
        "    for i,weight in enumerate(self.actor.weights):\n",
        "      weights.append(weight * tau + targets[i] *(1-tau))\n",
        "    self.target_actor.set_weights(weights)\n",
        "\n",
        "    weights = []\n",
        "    targets = self.target_critic.weights  \n",
        "    for i,weight in enumerate(self.critic.weights):\n",
        "      weights.append(weight * tau + targets[i] *(1-tau))\n",
        "    self.target_critic.set_weights(weights)\n",
        "\n",
        "  def store_transition(self,state,action,reward,state_,done):\n",
        "    self.memory.store_trans(state,action,reward,state_,done)\n",
        "\n",
        "  def save_models(self):\n",
        "    print('....saving models....')\n",
        "    self.actor.save_weights(self.act_name)\n",
        "    self.critic.save_weights(self.cri_name)\n",
        "    self.target_actor.save_weights(self.tar_act_name)\n",
        "    self.target_critic.save_weights(self.tar_cri_name)\n",
        "\n",
        "  def load_models(self):\n",
        "    print('....loading models....')\n",
        "    self.actor.load_weights(self.act_name)\n",
        "    self.critic.load_weights(self.cri_name)\n",
        "    self.target_actor.load_weights(self.tar_act_name)\n",
        "    self.target_critic.load_weights(self.tar_cri_name)\n",
        "\n",
        "  def choose_action(self,observation,evaluate = False):\n",
        "    state = tf.convert_to_tensor([observation])\n",
        "    probs = self.actor(state)\n",
        "    action_probs = tfp.distributions.Categorical(probs = probs)\n",
        "    action = action_probs.sample()\n",
        "    action = action.numpy()[0]\n",
        "    return action\n",
        "\n",
        "  def learn(self,agents):\n",
        "    if self.memory.mem_counter < self.batch_size:\n",
        "      return\n",
        "    \n",
        "    self.learnCounter += 1\n",
        "    agent_2 = agents[0]\n",
        "\n",
        "    samples = self.memory.returnSample(self.batch_size)\n",
        "\n",
        "    state_1,action_1,reward_1,new_state_1,done_1 = self.memory.sample_buffer(samples)\n",
        "    states_1 = tf.convert_to_tensor(state_1)\n",
        "    new_states_1 = tf.convert_to_tensor(new_state_1)\n",
        "    rewards_1 = tf.convert_to_tensor(reward_1, dtype= tf.float32)\n",
        "    actions_1 = tf.convert_to_tensor(action_1,dtype= tf.float32)\n",
        "\n",
        "    state_2,action_2,reward_2,new_state_2,done_2 = agent_2.memory.sample_buffer(samples)\n",
        "    states_2 = tf.convert_to_tensor(state_2)\n",
        "    new_states_2 = tf.convert_to_tensor(new_state_2)\n",
        "    rewards_2 = tf.convert_to_tensor(reward_2)\n",
        "    actions_2 = tf.convert_to_tensor(action_2,dtype= tf.float32)\n",
        "\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        target_actions_probs_1 = self.target_actor(new_states_1)\n",
        "        target_probs = tfp.distributions.Categorical(probs = target_actions_probs_1)\n",
        "        target_actions_1 = tf.transpose([target_probs.sample()])\n",
        "    \n",
        "        target_actions_probs_2 = agent_2.target_actor(new_states_2)\n",
        "        target_probs = tfp.distributions.Categorical(probs = target_actions_probs_2)\n",
        "        target_actions_2 = tf.transpose([target_probs.sample()])\n",
        "      \n",
        "\n",
        "        actions_1 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(actions_1), [0]))\n",
        "        actions_2 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(actions_2), [0]))\n",
        "        actions = tf.concat([actions_1,actions_2],axis = 1)\n",
        "        actions = tf.cast(actions,tf.float32)\n",
        "\n",
        "        new_states = tf.concat([new_states_1,new_states_2],axis = 1)\n",
        "        target_actions = tf.concat([target_actions_1,target_actions_2], axis = 1)\n",
        "        target_actions = tf.cast(target_actions,tf.float32)\n",
        "\n",
        "      \n",
        "        critic_value_ = tf.squeeze(self.target_critic((new_states,target_actions)),1)\n",
        "        states= tf.concat([states_1,states_2],axis = 1)\n",
        "     \n",
        "        critic_value = tf.squeeze(self.critic((states,actions)),1)\n",
        "        target = rewards_1 + self.gamma*critic_value_*(1-done_1)\n",
        "        critic_loss = keras.losses.MSE(target,critic_value)\n",
        "    \n",
        "    params = self.target_critic.trainable_variables\n",
        "    grads = tape.gradient(critic_loss,params)\n",
        "    self.critic.optimizer.apply_gradients(zip(grads,params))\n",
        "\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        new_policy_actions_probs = self.actor(states_1)\n",
        "        new_probs = tfp.distributions.Categorical(probs = new_policy_actions_probs)\n",
        "        new_actions_1 = tf.transpose([new_probs.sample()])\n",
        "\n",
        "        new_actions_probs_2 = agent_2.actor(states_2)\n",
        "        new_probs = tfp.distributions.Categorical(probs = new_actions_probs_2)\n",
        "        new_actions_2 = tf.transpose([new_probs.sample()])\n",
        "\n",
        "        new_actions = tf.concat([new_actions_1,new_actions_2],axis =1 )\n",
        "        new_actions = tf.cast(new_actions,tf.float32)\n",
        "        states = tf.concat([states_1,states_2],axis = 1)\n",
        "          \n",
        "        actor_loss = -self.critic((states,new_actions))\n",
        "        actor_loss = actor_loss[0]\n",
        "    \n",
        "    params = self.critic.trainable_variables\n",
        "    grads = tape.gradient(actor_loss,params)\n",
        "    self.actor.optimizer.apply_gradients(zip(grads,params))\n",
        "\n",
        "    if (self.learnCounter % 100 == 0):\n",
        "      self.Update_net_parameters()\n",
        "      self.learnCounter = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mfQlx_jpTuZ"
      },
      "outputs": [],
      "source": [
        "env.reset()\n",
        "agent_net = {}\n",
        "agent_list = env.agents\n",
        "for a in env.agents:\n",
        "  agent = Agent(input_dims= 12 ,alpha = 1e-5,env = env , n_actions = 5,name = a)\n",
        "  agent_net[a] = agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "V_VeWxC5qyhr",
        "outputId": "4e554ef0-0960-461a-ddc4-0480ef775418"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['agent_0', 'agent_1']\n",
            "....saving models....\n",
            "....saving models....\n",
            "episode:  0 score: -57.84  average score -57.84\n",
            "....saving models....\n",
            "....saving models....\n",
            "episode:  1 score: -57.05  average score -57.45\n",
            "episode:  2 score: -63.91  average score -59.60\n",
            "episode:  3 score: -51.28  average score -57.52\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-dfccd18e7c9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mlearning_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_net\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0magent_net\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_agents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0magent_net\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_transition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-bcdb11a23e03>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, agents)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearnCounter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[1;32m    680\u001b[0m           \u001b[0mstrategy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m           \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_distributed_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/merge_call_interim.py\u001b[0m in \u001b[0;36mmaybe_merge_call\u001b[0;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m   \"\"\"\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mstrategy_supports_no_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     return distribution_strategy_context.get_replica_context().merge_call(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_distributed_apply\u001b[0;34m(self, distribution, grads_and_vars, apply_state, name)\u001b[0m\n\u001b[1;32m    722\u001b[0m               var.op.name):\n\u001b[1;32m    723\u001b[0m             update_op = distribution.extended.update(\n\u001b[0;32m--> 724\u001b[0;31m                 var, apply_grad_to_update_var, args=(grad,), group=False)\n\u001b[0m\u001b[1;32m    725\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_cross_replica_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m               \u001b[0;31m# In cross-replica context, extended.update returns a list of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   2628\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[1;32m   2629\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2630\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2631\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2632\u001b[0m       return self._replica_ctx_update(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   3701\u001b[0m     \u001b[0;31m# The implementations of _update() and _update_non_slot() are identical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3702\u001b[0m     \u001b[0;31m# except _update() passes `var` as the first argument to `fn()`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3703\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_non_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3705\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_update_non_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_with\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_update_non_slot\u001b[0;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[1;32m   3707\u001b[0m     \u001b[0;31m# once that value is used for something.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3708\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mUpdateContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3709\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3710\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3711\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_grad_to_update_var\u001b[0;34m(var, grad)\u001b[0m\n\u001b[1;32m    704\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m\"apply_state\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_apply_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0mapply_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"apply_state\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m       \u001b[0mupdate_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resource_apply_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mapply_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstraint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py\u001b[0m in \u001b[0;36m_resource_apply_dense\u001b[0;34m(self, grad, var, apply_state)\u001b[0m\n\u001b[1;32m    179\u001b[0m           \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoefficients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epsilon'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m           \u001b[0mgrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m           use_locking=self._use_locking)\n\u001b[0m\u001b[1;32m    182\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m       \u001b[0mvhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vhat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/tf_export.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m           \u001b[0;34m'Please pass these args as kwargs instead.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m           .format(f=f.__name__, kwargs=f_argspec.args))\n\u001b[0;32m--> 400\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorator_argspec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf_argspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_training_ops.py\u001b[0m in \u001b[0;36mresource_apply_adam\u001b[0;34m(var, m, v, beta1_power, beta2_power, lr, beta1, beta2, epsilon, grad, use_locking, use_nesterov, name)\u001b[0m\n\u001b[1;32m   1422\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ResourceApplyAdam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta1_power\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2_power\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_locking\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1424\u001b[0;31m         \"use_nesterov\", use_nesterov)\n\u001b[0m\u001b[1;32m   1425\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "n_games = 2500\n",
        "best_score = -1000\n",
        "score_history = []\n",
        "print(agent_list)\n",
        "load_checkpoint = False\n",
        "if load_checkpoint:\n",
        "  for a in agent_list:\n",
        "    agent_net[a].load_models()\n",
        "else:\n",
        "  evaluate = False\n",
        "for i in range(n_games):\n",
        "  score = 0\n",
        "  env.reset(seed = 3)\n",
        "  for agent in env.agent_iter():\n",
        "    state,reward,done,trunc,info = env.last()\n",
        "    if not (done or trunc):\n",
        "      action = agent_net[agent].choose_action(state,evaluate)\n",
        "      env.step(action)\n",
        "      score += reward\n",
        "      state_,_,_,_,_ = env.last()\n",
        "    else:\n",
        "      env.step(None)\n",
        "    learning_agents = []\n",
        "    for a in agent_list:\n",
        "      if (a != agent):\n",
        "        learning_agents.append(agent_net[a])  \n",
        "    agent_net[agent].learn(learning_agents)\n",
        "    agent_net[agent].store_transition(state,action,reward,state_,done)\n",
        "  \n",
        "  score_history.append(score)\n",
        "  avg_score = np.mean(score_history[max(0, i-100):(i+1)])\n",
        "\n",
        "  if avg_score > best_score:\n",
        "    best_score = avg_score\n",
        "    if not load_checkpoint:\n",
        "      for a in agent_list:\n",
        "        agent_net[a].save_models()\n",
        "  \n",
        "  \n",
        "  print('episode: ', i,'score: %.2f' % score,' average score %.2f' % avg_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2368h6WqW34X"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import savgol_filter\n",
        "\n",
        "x = np.linspace(0,2*np.pi,100)\n",
        "y = np.sin(x) + np.random.random(100) * 0.2\n",
        "yhat = savgol_filter(score_history, 101, 7) # window size 51, polynomial order 3\n",
        "\n",
        "\n",
        "plt.plot(yhat, color='red')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSGoC__asq3W"
      },
      "outputs": [],
      "source": [
        "print(score_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKeiqFVIPQ3c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}