{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luthandonx/Multi-Agent-Reinforcement-Learining/blob/Simple-Listener-Speaker/Maddpg_Simple_Speaker_Listiner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iEqcrFVZ4NY"
      },
      "outputs": [],
      "source": [
        "!pip install pettingzoo[mpe]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OD03tGzbH5i"
      },
      "outputs": [],
      "source": [
        "from pettingzoo.mpe import simple_speaker_listener_v3\n",
        "env = simple_speaker_listener_v3.env(max_cycles = 25 , continuous_actions = False)\n",
        "env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_42jRaUbMAy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Replay():\n",
        "  def __init__(self,max_size,n_actions,input_shape):\n",
        "    self.mem_size = max_size\n",
        "    self.mem_counter = 0\n",
        "    self.state_memory = np.zeros((self.mem_size,input_shape))\n",
        "    self.new_state_memory = np.zeros((self.mem_size,input_shape))\n",
        "    self.action_memory = np.zeros((self.mem_size,n_actions))\n",
        "    self.reward_memory = np.zeros(self.mem_size)\n",
        "    self.terminal_memory = np.zeros(self.mem_size,dtype = bool)\n",
        "\n",
        "  def store_trans(self,state,action,reward,done):\n",
        "    index = self.mem_counter % self.mem_size\n",
        "    self.state_memory[index] = state\n",
        "    self.action_memory[index] = action\n",
        "    self.reward_memory[index] = reward\n",
        "    self.terminal_memory[index] = done\n",
        "\n",
        "    if (self.mem_counter > 0):\n",
        "      self.new_state_memory[index-1] = state\n",
        "    self.mem_counter += 1\n",
        "\n",
        "  def sample_buffer(self,batch):\n",
        "    self.batch = batch\n",
        "    states = self.state_memory[batch]\n",
        "    actions = self.action_memory[batch]\n",
        "    rewards = self.reward_memory[batch]\n",
        "    states_ = self.new_state_memory[batch]\n",
        "    dones = self.terminal_memory[batch]\n",
        "    \n",
        "    return   states,actions,rewards,states_,dones\n",
        "\n",
        "  def returnSample(self,batch_size):\n",
        "    max_mem = min(batch_size,self.mem_counter)\n",
        "    batch = np.random.choice(max_mem,batch_size) \n",
        "    return batch\n",
        "\n",
        "  def returnshit(self):\n",
        "    return self.state_memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MP5hWqb0bOeq"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "class CriticNet(keras.Model):\n",
        "  def __init__(self,n_actions, dim_layer):\n",
        "    super(CriticNet,self).__init__()\n",
        "    self.dim_size = dim_layer\n",
        "    self.n_actions = n_actions\n",
        "    self.dim_size = 128\n",
        "    self.layer_one = Dense(self.dim_size,activation='relu')\n",
        "    self.layer_two = Dense(self.dim_size,activation='relu')\n",
        "    self.q_value = Dense(1,activation=None)\n",
        "\n",
        "  def call(self,inputs):\n",
        "    states,actions = inputs\n",
        "    input_thing = tf.concat([states, actions],axis = 1)\n",
        "    action_value = self.layer_one(input_thing)\n",
        "    action_value = self.layer_two(action_value)\n",
        "\n",
        "    q = self.q_value(action_value)\n",
        "\n",
        "    return q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fA1K5G2GbQ8S"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "class ActorNet(keras.Model):\n",
        "  def __init__(self,n_actions, dim_layer):\n",
        "    super(ActorNet,self).__init__()\n",
        "    self.dim_size = dim_layer\n",
        "    self.n_actions = n_actions\n",
        "\n",
        "    self.layer_one = Dense(self.dim_size, activation = 'relu')\n",
        "    self.layer_two = Dense(self.dim_size, activation = 'relu')\n",
        "    self.policy = Dense(self.n_actions, activation = 'softmax')\n",
        "\n",
        "  def call(self,state):\n",
        "    probs = self.layer_one(state)\n",
        "    probs = self.layer_two(probs)\n",
        "\n",
        "    mu = self.policy(probs)\n",
        "    return mu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7roZFouwbWlT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "\n",
        "class Agent:\n",
        "  def __init__(self,input_dims,alpha = 0.01,beta =1e-5 , env = None ,\n",
        "               gamma = 0.95, n_actions = 5, max_size = 10000, tau = 0.001,\n",
        "               layer_size = 256, batch_size = 50,noise = 0.1, name = None):\n",
        "    \n",
        "    self.gamma = gamma\n",
        "    self.tau = tau\n",
        "    self.memory = Replay(max_size,n_actions,input_dims)\n",
        "    self.batch_size = batch_size\n",
        "    self.n_actions = n_actions\n",
        "    self.act_name = name+' actor'\n",
        "    self.cri_name = name+' critc'\n",
        "    self.tar_act_name = name+' target_actor'\n",
        "    self.tar_cri_name = name+' target_critc'\n",
        "    \n",
        "    self.actor = ActorNet(n_actions = n_actions, dim_layer = layer_size)\n",
        "    self.critic = CriticNet(n_actions,layer_size)\n",
        "\n",
        "    self.target_actor = ActorNet(n_actions = n_actions, dim_layer = layer_size)\n",
        "    self.target_critic = CriticNet(n_actions,layer_size)\n",
        "\n",
        "    self.actor.compile(keras.optimizers.Adam(learning_rate=alpha))\n",
        "    self.target_actor.compile(keras.optimizers.Adam(learning_rate=alpha))\n",
        "\n",
        "    self.critic.compile(keras.optimizers.Adam(learning_rate=beta))\n",
        "    self.target_critic.compile(keras.optimizers.Adam(learning_rate=beta))\n",
        "\n",
        "    self.learnCounter = 0\n",
        "\n",
        "  def Update_net_parameters(self,tau=None):\n",
        "    if tau is None:\n",
        "      tau = self.tau\n",
        "    \n",
        "    weights = []\n",
        "    targets = self.target_actor.weights\n",
        "\n",
        "    for i,weight in enumerate(self.actor.weights):\n",
        "      weights.append(weight * tau + targets[i] *(1-tau))\n",
        "    self.target_actor.set_weights(weights)\n",
        "\n",
        "    weights = []\n",
        "    targets = self.target_critic.weights  \n",
        "    for i,weight in enumerate(self.critic.weights):\n",
        "      weights.append(weight * tau + targets[i] *(1-tau))\n",
        "    self.target_critic.set_weights(weights)\n",
        "\n",
        "  def store_transition(self,state,action,reward,done):\n",
        "    self.memory.store_trans(state,action,reward,done)\n",
        "\n",
        "  def save_models(self):\n",
        "    print('....saving models....')\n",
        "    self.actor.save_weights(self.act_name)\n",
        "    self.critic.save_weights(self.cri_name)\n",
        "    self.target_actor.save_weights(self.tar_act_name)\n",
        "    self.target_critic.save_weights(self.tar_cri_name)\n",
        "\n",
        "    self.actor.save(self.act_name)\n",
        "\n",
        "  def load_models(self):\n",
        "    print('....loading models....')\n",
        "    self.actor.load_weights(self.act_name)\n",
        "    self.critic.load_weights(self.cri_name)\n",
        "    self.target_actor.load_weights(self.tar_act_name)\n",
        "    self.target_critic.load_weights(self.tar_cri_name)\n",
        "\n",
        "  def choose_action(self,observation,evaluate = False):\n",
        "    state = tf.convert_to_tensor([observation])\n",
        "    probs = self.actor(state)\n",
        "    action_probs = tfp.distributions.Categorical(probs = probs)\n",
        "    action = action_probs.sample()\n",
        "    action = action.numpy()[0]\n",
        "    return action\n",
        "\n",
        "  def learn(self,agents):\n",
        "    \n",
        "    if self.memory.mem_counter < self.batch_size:\n",
        "      return\n",
        "    \n",
        "    self.learnCounter += 1\n",
        "    agent_2 = agents[0]\n",
        "\n",
        "    samples = self.memory.returnSample(self.batch_size)\n",
        "\n",
        "    state_1,action_1,reward_1,new_state_1,done_1 = self.memory.sample_buffer(samples)\n",
        "    states_1 = tf.convert_to_tensor(state_1)\n",
        "    new_states_1 = tf.convert_to_tensor(new_state_1)\n",
        "    rewards_1 = tf.convert_to_tensor(reward_1, dtype= tf.float32)\n",
        "    actions_1 = tf.convert_to_tensor(action_1,dtype= tf.float32)\n",
        "\n",
        "    state_2,action_2,reward_2,new_state_2,done_2 = agent_2.memory.sample_buffer(samples)\n",
        "    states_2 = tf.convert_to_tensor(state_2)\n",
        "    new_states_2 = tf.convert_to_tensor(new_state_2)\n",
        "    rewards_2 = tf.convert_to_tensor(reward_2)\n",
        "    actions_2 = tf.convert_to_tensor(action_2,dtype= tf.float32)\n",
        "\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        target_actions_probs_1 = self.target_actor(new_states_1)\n",
        "        target_probs = tfp.distributions.Categorical(probs = target_actions_probs_1)\n",
        "        target_actions_1 = tf.transpose([target_probs.sample()])\n",
        "\n",
        "    \n",
        "        target_actions_probs_2 = agent_2.target_actor(new_states_2)\n",
        "        target_probs = tfp.distributions.Categorical(probs = target_actions_probs_2)\n",
        "        target_actions_2 = tf.transpose([target_probs.sample()])\n",
        "      \n",
        "\n",
        "        actions_1 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(actions_1), [0]))\n",
        "        actions_2 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(actions_2), [0]))\n",
        "        actions = tf.concat([actions_1,actions_2],axis = 1)\n",
        "        actions = tf.cast(actions,tf.float32)\n",
        "\n",
        "        new_states = tf.concat([new_states_1,new_states_2],axis = 1)\n",
        "        target_actions = tf.concat([target_actions_1,target_actions_2], axis = 1)\n",
        "        target_actions = tf.cast(target_actions,tf.float32)\n",
        "\n",
        "      \n",
        "        critic_value_ = tf.squeeze(self.target_critic((new_states,target_actions)),1)\n",
        "        states= tf.concat([states_1,states_2],axis = 1)\n",
        "     \n",
        "        critic_value = tf.squeeze(self.critic((states,actions)),1)\n",
        "        target = rewards_1 + self.gamma*critic_value_*(1-done_1)\n",
        "        critic_loss = keras.losses.MSE(target,critic_value)\n",
        "    \n",
        "    params = self.target_critic.trainable_variables\n",
        "    grads = tape.gradient(critic_loss,params)\n",
        "    self.critic.optimizer.apply_gradients(zip(grads,params))\n",
        "\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        new_policy_actions_probs = self.actor(states_1)\n",
        "        new_probs = tfp.distributions.Categorical(probs = new_policy_actions_probs)\n",
        "        new_actions_1 = tf.transpose([new_probs.sample()])\n",
        "\n",
        "        new_actions_probs_2 = agent_2.actor(states_2)\n",
        "        new_probs = tfp.distributions.Categorical(probs = new_actions_probs_2)\n",
        "        new_actions_2 = tf.transpose([new_probs.sample()])\n",
        "\n",
        "        new_actions = tf.concat([new_actions_1,new_actions_2],axis =1 )\n",
        "        new_actions = tf.cast(new_actions,tf.float32)\n",
        "        states = tf.concat([states_1,states_2],axis = 1)\n",
        "          \n",
        "        actor_loss = -self.critic((states,new_actions))\n",
        "        actor_loss = actor_loss[0]\n",
        "    \n",
        "    params = self.critic.trainable_variables\n",
        "    grads = tape.gradient(actor_loss,params)\n",
        "    self.actor.optimizer.apply_gradients(zip(grads,params))\n",
        "\n",
        "    if (self.learnCounter % 100 == 0):\n",
        "      self.Update_net_parameters()\n",
        "      self.learnCounter = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9-i5X1tco8z"
      },
      "outputs": [],
      "source": [
        "agent_net = {}\n",
        "agent_list = []\n",
        "for a in env.agents:\n",
        "  obsv = env.observation_space(a).shape[0]\n",
        "  n_action = env.action_space(a).n\n",
        "  agent = Agent(input_dims = obsv, alpha = 1e-5,beta = 1e-5, env = env, gamma = 0.95,n_actions = n_action, max_size = 10000,\n",
        "                layer_size = 256,batch_size = 64,noise = 0.1, name = a)\n",
        "  agent_list.append(a)\n",
        "  agent_net[a] = agent "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGVoyeAbedht"
      },
      "outputs": [],
      "source": [
        "n_games = 1000\n",
        "score_history = []\n",
        "listener_state_ = []\n",
        "speaker_state_ = []\n",
        "best_score = -1000\n",
        "prev_state = []\n",
        "\n",
        "avg_scores = []\n",
        "speaker = False\n",
        "for i in range(n_games):\n",
        "  score = 0\n",
        "  env.reset()\n",
        "  for agent in env.agent_iter():\n",
        "    state,reward,done,trunc,_ = env.last()\n",
        "    prev_state.append(state)\n",
        "    if not (done or trunc):\n",
        "      if (agent == 'speaker_0'):\n",
        "        action = agent_net[agent].choose_action(state)\n",
        "      else: \n",
        "        action = agent_net[agent].choose_action(state) \n",
        "    else:\n",
        "      action = (None)\n",
        "    score += reward \n",
        "    env.step(action)\n",
        "    agent_net[agent].store_transition(state,action,reward,done)\n",
        "\n",
        "    agent_net['speaker_0'].learn([agent_net['listener_0']])\n",
        "    agent_net['listener_0'].learn([agent_net['speaker_0']])\n",
        "  \n",
        "\n",
        "  score_history.append(score)\n",
        "\n",
        "  avg_score = np.mean(score_history[-100:])\n",
        "\n",
        "  if avg_score > best_score:\n",
        "    best_score = avg_score\n",
        "    agent_net['speaker_0'].save_models()\n",
        "    agent_net['speaker_0'].save_models() \n",
        "  \n",
        "  print('episode: ', i,'average score: %.2f' % avg_score)\n",
        "  avg_scores.append(avg_score)\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJCPDzDi4Muc"
      },
      "outputs": [],
      "source": [
        "print(avg_scores)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyOtoMcBfV3VSHXOZixUN3d4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}