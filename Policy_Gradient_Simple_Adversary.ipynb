{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMHGfW3AdphfV4g64tAh+H9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luthandonx/Multi-Agent-Reinforcement-Learining/blob/Simple-Adversary/Policy_Gradient_Simple_Adversary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vDMUCyBNGiR"
      },
      "outputs": [],
      "source": [
        "!pip install pettingzoo[mpe]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input,Dense,Activation\n",
        "from keras.models import Model,load_model\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import keras.backend as K\n",
        "\n",
        "class Agent(object):\n",
        "  def __init__ (self,name,alpha,gamma,input_dims, n_actions,layer_size,fname ='reinforcePolicy'):\n",
        "    self.gamma = gamma\n",
        "    self.lr = alpha\n",
        "    self.G = 0\n",
        "    self.input_dims = input_dims\n",
        "    self.layer_size = layer_size\n",
        "    self.n_actions = n_actions\n",
        "    self.state_memory = []\n",
        "    self.action_memory = []\n",
        "    self.reward_memory = []\n",
        "    self.agent_name = name\n",
        "\n",
        "\n",
        "    self.policy,self.predict = self.create_policy()\n",
        "    self.action_space = [i for i in range(n_actions)]\n",
        "    self.model_file = fname+' '+self.agent_name\n",
        "\n",
        "  def create_policy(self):\n",
        "    input = Input(shape = (self.input_dims,))#comma indicates that it takes a batch\n",
        "    advantages = Input(shape =[1])\n",
        "    dense1 = Dense(self.layer_size,activation = 'relu')(input)\n",
        "    dense2 = Dense(self.layer_size,activation = 'relu')(dense1)\n",
        "    probs = Dense(self.n_actions, activation = 'softmax')(dense2)\n",
        "\n",
        "    def custom_loss(y_true,y_pred):\n",
        "      out = K.clip(y_pred,1e-5, 1-1e-5) #this is to ensure that we do not perform log calcualtions with log values of 0\n",
        "      log_lik = y_true*K.log(out)\n",
        "\n",
        "      return K.sum(-log_lik*advantages)\n",
        "    \n",
        "    policy = Model(inputs = [input,advantages], outputs = [probs])\n",
        "    opt = keras.optimizers.Adam(learning_rate = self.lr)\n",
        "    policy.compile(optimizer = opt, loss = custom_loss)\n",
        "\n",
        "    predict = keras.Model(inputs = [input], outputs = [probs])\n",
        "\n",
        "    return policy,predict\n",
        "\n",
        "  def choose_action(self,obsv): \n",
        "    obsv = np.expand_dims(obsv,0)\n",
        "    #to keep the input shape consistant\n",
        "    probabilities = self.predict.predict(obsv)[0]\n",
        "    # we take the 0th element because predict returns a tuple\n",
        "    action = np.random.choice(self.action_space,p=probabilities)\n",
        "    # action = np.argmax(probabilities)\n",
        "    return action\n",
        "\n",
        "  def store_trans(self,obsv,action,reward):\n",
        "    self.action_memory.append(action)\n",
        "    self.state_memory.append(obsv)\n",
        "    self.reward_memory.append(reward)\n",
        "\n",
        "  def learn(self):\n",
        "    state_memory = np.array(self.state_memory)\n",
        "    reward_memory = np.array(self.reward_memory)\n",
        "    action_memory = np.array(self.action_memory)\n",
        "\n",
        "\n",
        "    actions = np.zeros([len(action_memory),self.n_actions])\n",
        "    actions[np.arange(len(action_memory)),action_memory] = 1\n",
        "    # loss function requires the labels to be 1 hot encoded\n",
        "\n",
        "    G = np.zeros_like(reward_memory)\n",
        "    for t in range(len(reward_memory)):\n",
        "      G_sum = 0\n",
        "      discount = 1\n",
        "      for k in range(t,len(reward_memory)):\n",
        "        G_sum += reward_memory[k]*discount\n",
        "        discount *= self.gamma\n",
        "      G[t] = G_sum\n",
        "\n",
        "    mean = np.mean(G)\n",
        "    std = np.std(G) if np.std(G) > 0 else 1\n",
        "    self.G = (G-mean)/std\n",
        "    print(len(self.G))\n",
        "    print(len(state_memory))\n",
        "    print(len(actions))\n",
        "    cost = self.policy.train_on_batch([state_memory,self.G], actions) # actions is our labels\n",
        "    #[state_memory,self.G] =  y_pred and actions = y_true\n",
        "    # This is to calculate for the loss function\n",
        "\n",
        "    self.state_memory = []\n",
        "    self.reward_memory = []\n",
        "    self.action_memory = []\n",
        "\n",
        "  def save_model(self):\n",
        "    self.policy.save_weights(self.model_file)\n",
        "    self.policy.save(self.model_file)\n",
        "\n",
        "  def load_model(self):\n",
        "    self.policy.load_weights(self.model_file)"
      ],
      "metadata": {
        "id": "xcnAsMKvO6GZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "\n",
        "disable_eager_execution()"
      ],
      "metadata": {
        "id": "3loktP3BvcOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pettingzoo.mpe import simple_adversary_v2\n",
        "env = simple_adversary_v2.env(N=2,max_cycles = 25 , continuous_actions = False)\n",
        "env.reset()"
      ],
      "metadata": {
        "id": "dV_7gQsKOfAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_net = {}\n",
        "agent_list = []\n",
        "for a in env.agents:\n",
        "  obs_space = env.observation_space(a).shape\n",
        "  print(obs_space[0])\n",
        "  agent = Agent(a, 1e-5 , 0.99 , obs_space[0] ,5,256)\n",
        "  agent_net[a] = agent\n",
        "  agent_list.append(a)"
      ],
      "metadata": {
        "id": "1uC-vZWhh2Uf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fedb82c5-2144-493c-e214-1f9b3ab50805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n",
            "10\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()\n",
        "max_episode = 1000\n",
        "good_best_score = -1000\n",
        "opp_best_score = -10000\n",
        "Good_score_history = []\n",
        "Opp_score_history = []\n",
        "good_score = 0\n",
        "opp_score  = 0\n",
        "Good_avg_score = []\n",
        "Opp_score_history = []\n",
        "\n",
        "for i in range(max_episode):\n",
        "  opp_score = 0\n",
        "  good_score = 0\n",
        "  env.reset()\n",
        "  for agent in env.agent_iter():\n",
        "    observation,reward,done,trunc,_= env.last()\n",
        "    if (agent == 'adversary_0'):\n",
        "      opp_score += reward\n",
        "    else:\n",
        "      good_score += reward \n",
        "    if not (done or trunc):\n",
        "      action = agent_net[agent].choose_action(observation)\n",
        "      env.step(action)\n",
        "      agent_net[agent].store_trans(observation,action,reward)\n",
        "    else:\n",
        "      action = None\n",
        "      env.step(action)\n",
        "\n",
        "  Good_score_history.append(good_score)\n",
        "  Opp_score_history.append(opp_score)\n",
        "\n",
        "  Good_avg_score = np.mean(Good_score_history[-100:])\n",
        "  Opp_avg_score = np.mean(Opp_score_history[-100:])\n",
        "\n",
        "  for agent in env.agents:\n",
        "    agent_net[agent].learn()\n",
        "  \n",
        "  if Good_avg_score > good_best_score:\n",
        "    load_checkpoint = True\n",
        "    good_best_score = Good_avg_score\n",
        "    for a in agent_list:\n",
        "      if (a != 'adversary_0'):\n",
        "        agent_net[a].save_model()\n",
        "\n",
        "  if Opp_avg_score > opp_best_score:\n",
        "    load_checkpoint = True\n",
        "    opp_best_score = Opp_avg_score\n",
        "    for a in agent_list:\n",
        "      if (a == 'adversary_0'):\n",
        "        agent_net[a].save_model()\n",
        " \n",
        "\n",
        "\n",
        "  print('episode: ', i ,' Opp avg score:', Opp_avg_score,'Good avg score :',Good_avg_score)"
      ],
      "metadata": {
        "id": "cHQlUqmaosSm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "78f6d5fe-39c8-4469-8e75-5db68004e1c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode:  0  Opp avg score: -17.73136877892052 Good avg score : -15.820822571087099\n",
            "episode:  1  Opp avg score: -14.774178950045151 Good avg score : -8.43626675544986\n",
            "episode:  2  Opp avg score: -26.3690510889497 Good avg score : 12.91410715224606\n",
            "episode:  3  Opp avg score: -23.887606908238222 Good avg score : 1.6586724781292492\n",
            "episode:  4  Opp avg score: -26.8490767426516 Good avg score : 2.1097004981065615\n",
            "episode:  5  Opp avg score: -25.453988725890284 Good avg score : -1.2198243166113194\n",
            "episode:  6  Opp avg score: -27.190126388350986 Good avg score : 0.5847595192566709\n",
            "episode:  7  Opp avg score: -27.73157435512867 Good avg score : 3.239515933040388\n",
            "episode:  8  Opp avg score: -27.679839220404162 Good avg score : 7.185847056682885\n",
            "episode:  9  Opp avg score: -26.73341537010581 Good avg score : 6.885403819832108\n",
            "episode:  10  Opp avg score: -26.023473550502942 Good avg score : 3.868634055201302\n",
            "episode:  11  Opp avg score: -28.113352845058596 Good avg score : 6.575467109333112\n",
            "episode:  12  Opp avg score: -28.536361202858984 Good avg score : 9.974991132189254\n",
            "episode:  13  Opp avg score: -27.457890300978057 Good avg score : 9.651273338623225\n",
            "episode:  14  Opp avg score: -26.93064341251901 Good avg score : 6.453903152432141\n",
            "episode:  15  Opp avg score: -27.941479089097733 Good avg score : 10.348236479485937\n",
            "episode:  16  Opp avg score: -28.03143175907995 Good avg score : 9.422945288166577\n",
            "episode:  17  Opp avg score: -28.112297667243197 Good avg score : 8.892123743785048\n",
            "episode:  18  Opp avg score: -28.274726153140147 Good avg score : 9.984125445412955\n",
            "episode:  19  Opp avg score: -28.488113528586986 Good avg score : 9.824611884252302\n",
            "episode:  20  Opp avg score: -28.920803743515847 Good avg score : 11.20373183740435\n",
            "episode:  21  Opp avg score: -28.93905341345604 Good avg score : 10.908552991219407\n",
            "episode:  22  Opp avg score: -29.239173648597934 Good avg score : 11.898259297491071\n",
            "episode:  23  Opp avg score: -29.259372400085997 Good avg score : 12.83109432961149\n",
            "episode:  24  Opp avg score: -29.474840078822456 Good avg score : 12.329777942910988\n",
            "episode:  25  Opp avg score: -29.976304150728065 Good avg score : 13.341218746889199\n",
            "episode:  26  Opp avg score: -29.828140364302886 Good avg score : 14.000975644575975\n",
            "episode:  27  Opp avg score: -30.592147257800182 Good avg score : 16.070448602672467\n",
            "episode:  28  Opp avg score: -30.85325471442349 Good avg score : 16.429948659129227\n",
            "episode:  29  Opp avg score: -30.184299063406588 Good avg score : 16.209214216817557\n",
            "episode:  30  Opp avg score: -30.253775448510364 Good avg score : 16.047779245892738\n",
            "episode:  31  Opp avg score: -29.933129928606107 Good avg score : 16.00760651219318\n",
            "episode:  32  Opp avg score: -29.937893333211633 Good avg score : 15.826022642779066\n",
            "episode:  33  Opp avg score: -30.089619108730066 Good avg score : 15.54011311235525\n",
            "episode:  34  Opp avg score: -29.692399334394075 Good avg score : 14.872052724029459\n",
            "episode:  35  Opp avg score: -29.347869536239003 Good avg score : 14.551881239781153\n",
            "episode:  36  Opp avg score: -29.084008773397525 Good avg score : 14.088935239771272\n",
            "episode:  37  Opp avg score: -28.83833655768572 Good avg score : 14.031339492507126\n",
            "episode:  38  Opp avg score: -28.719558117888216 Good avg score : 14.07838470798219\n",
            "episode:  39  Opp avg score: -28.28705960051631 Good avg score : 12.364913795933152\n",
            "episode:  40  Opp avg score: -28.393994148187474 Good avg score : 12.793527648207126\n",
            "episode:  41  Opp avg score: -28.48724331439522 Good avg score : 12.151262837164698\n",
            "episode:  42  Opp avg score: -28.958290106820794 Good avg score : 13.652274385061663\n",
            "episode:  43  Opp avg score: -29.421578147829813 Good avg score : 14.43733866768464\n",
            "episode:  44  Opp avg score: -29.606028160435976 Good avg score : 14.496786469802228\n",
            "episode:  45  Opp avg score: -29.847105820668187 Good avg score : 14.939343013179165\n",
            "episode:  46  Opp avg score: -29.310428112186457 Good avg score : 13.677814798573603\n",
            "episode:  47  Opp avg score: -29.28881528512939 Good avg score : 14.050929251201618\n",
            "episode:  48  Opp avg score: -29.41601559277506 Good avg score : 14.91378407917569\n",
            "episode:  49  Opp avg score: -29.740010757288438 Good avg score : 15.930866129944206\n",
            "episode:  50  Opp avg score: -29.45644373349981 Good avg score : 14.502336474326048\n",
            "episode:  51  Opp avg score: -29.058562897580337 Good avg score : 13.795740483312528\n",
            "episode:  52  Opp avg score: -28.95418904435279 Good avg score : 14.032651957686662\n",
            "episode:  53  Opp avg score: -28.706712082561992 Good avg score : 12.63381126707318\n",
            "episode:  54  Opp avg score: -28.70149145800236 Good avg score : 11.990613933381285\n",
            "episode:  55  Opp avg score: -28.44342490119705 Good avg score : 11.509242148213934\n",
            "episode:  56  Opp avg score: -28.704584456427146 Good avg score : 12.507802798834968\n",
            "episode:  57  Opp avg score: -28.854467350390387 Good avg score : 12.619534593580726\n",
            "episode:  58  Opp avg score: -28.875031301476874 Good avg score : 12.651082348044177\n",
            "episode:  59  Opp avg score: -28.740363552512864 Good avg score : 12.230756026439424\n",
            "episode:  60  Opp avg score: -28.425966328084325 Good avg score : 11.088102551638922\n",
            "episode:  61  Opp avg score: -28.189605648861708 Good avg score : 11.101986700440188\n",
            "episode:  62  Opp avg score: -28.61217929546903 Good avg score : 11.706561701465798\n",
            "episode:  63  Opp avg score: -28.84960831323802 Good avg score : 12.156239364995987\n",
            "episode:  64  Opp avg score: -28.88688424932609 Good avg score : 11.799252923264207\n",
            "episode:  65  Opp avg score: -28.89337043011913 Good avg score : 12.037379932473907\n",
            "episode:  66  Opp avg score: -29.13083193456693 Good avg score : 12.250533658970886\n",
            "episode:  67  Opp avg score: -29.219074267600703 Good avg score : 11.757198894187114\n",
            "episode:  68  Opp avg score: -29.005765501005726 Good avg score : 11.713497843526612\n",
            "episode:  69  Opp avg score: -29.22251377571297 Good avg score : 12.24816423325739\n",
            "episode:  70  Opp avg score: -29.05512445737872 Good avg score : 11.937243096293653\n",
            "episode:  71  Opp avg score: -29.047808423219017 Good avg score : 11.988482321499946\n",
            "episode:  72  Opp avg score: -29.205133509995143 Good avg score : 12.55696626327812\n",
            "episode:  73  Opp avg score: -29.31554184878134 Good avg score : 12.62939648010551\n",
            "episode:  74  Opp avg score: -29.24992257461974 Good avg score : 12.329920590951746\n",
            "episode:  75  Opp avg score: -29.38469693036985 Good avg score : 12.715314666651645\n",
            "episode:  76  Opp avg score: -29.459387364022778 Good avg score : 12.247650712231563\n",
            "episode:  77  Opp avg score: -29.466633417194988 Good avg score : 12.517156996498487\n",
            "episode:  78  Opp avg score: -29.575445471101524 Good avg score : 12.321736521119563\n",
            "episode:  79  Opp avg score: -29.625005148878824 Good avg score : 12.360525316749671\n",
            "episode:  80  Opp avg score: -29.52373853750362 Good avg score : 12.396477745877704\n",
            "episode:  81  Opp avg score: -29.82723729287649 Good avg score : 13.155817504016607\n",
            "episode:  82  Opp avg score: -29.581534173312686 Good avg score : 13.044170438051989\n",
            "episode:  83  Opp avg score: -29.416709703371172 Good avg score : 12.648485215831919\n",
            "episode:  84  Opp avg score: -29.254367700876937 Good avg score : 12.162472363561667\n",
            "episode:  85  Opp avg score: -29.42250888625528 Good avg score : 12.74945612567324\n",
            "episode:  86  Opp avg score: -29.45103397277171 Good avg score : 12.928282624132583\n",
            "episode:  87  Opp avg score: -29.358975583307505 Good avg score : 13.134614074704889\n",
            "episode:  88  Opp avg score: -29.156014845211185 Good avg score : 12.875689566057174\n",
            "episode:  89  Opp avg score: -29.297431456215083 Good avg score : 13.07983157965971\n",
            "episode:  90  Opp avg score: -29.385271179358778 Good avg score : 13.249608130913591\n",
            "episode:  91  Opp avg score: -29.56128375519803 Good avg score : 13.888201714709561\n",
            "episode:  92  Opp avg score: -29.561357124256563 Good avg score : 14.086878831766997\n",
            "episode:  93  Opp avg score: -29.478914688304403 Good avg score : 14.14237345916448\n",
            "episode:  94  Opp avg score: -29.421907900676587 Good avg score : 14.184869472936132\n",
            "episode:  95  Opp avg score: -29.345105404403824 Good avg score : 13.910970643374887\n",
            "episode:  96  Opp avg score: -29.349553889187288 Good avg score : 13.57902964670235\n",
            "episode:  97  Opp avg score: -29.42341806289349 Good avg score : 13.816208994020075\n",
            "episode:  98  Opp avg score: -29.253102455699498 Good avg score : 13.352857756895322\n",
            "episode:  99  Opp avg score: -29.31222453131637 Good avg score : 13.283160286596349\n",
            "episode:  100  Opp avg score: -29.444831034998927 Good avg score : 13.541775844840604\n",
            "episode:  101  Opp avg score: -29.9485652544017 Good avg score : 14.531507712962568\n",
            "episode:  102  Opp avg score: -29.56893258932895 Good avg score : 13.788485096986463\n",
            "episode:  103  Opp avg score: -29.86824529478627 Good avg score : 14.414607641017806\n",
            "episode:  104  Opp avg score: -30.010807908014534 Good avg score : 14.994832624495496\n",
            "episode:  105  Opp avg score: -30.40224100041593 Good avg score : 15.655213530649151\n",
            "episode:  106  Opp avg score: -30.10865850394509 Good avg score : 14.890952573093632\n",
            "episode:  107  Opp avg score: -29.998759951386557 Good avg score : 14.794035623771038\n",
            "episode:  108  Opp avg score: -29.894286438593834 Good avg score : 14.217898284958075\n",
            "episode:  109  Opp avg score: -30.19048510336795 Good avg score : 14.959250718679606\n",
            "episode:  110  Opp avg score: -30.26232049747862 Good avg score : 15.552193116841543\n",
            "episode:  111  Opp avg score: -30.05978447699449 Good avg score : 15.420780292890367\n",
            "episode:  112  Opp avg score: -29.91403472780522 Good avg score : 14.941869196186504\n",
            "episode:  113  Opp avg score: -30.02057363231578 Good avg score : 14.987355754136804\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-bd8f7f4b9b0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent_net\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m       \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m       \u001b[0magent_net\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_trans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pettingzoo/utils/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pettingzoo/utils/wrappers/base.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_selection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pettingzoo/utils/wrappers/assert_out_of_bounds.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         ), \"action is not in action space\"\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pettingzoo/utils/wrappers/base.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_selection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pettingzoo/mpe/_mpe_utils/simple_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnext_idx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_world_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_cycles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pettingzoo/mpe/_mpe_utils/simple_env.py\u001b[0m in \u001b[0;36m_execute_world_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0magent_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscenario\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_ratio\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 reward = (\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pettingzoo/mpe/simple_adversary/simple_adversary.py\u001b[0m in \u001b[0;36mreward\u001b[0;34m(self, agent, world)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madversary_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworld\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madversary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworld\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         )\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pettingzoo/mpe/simple_adversary/simple_adversary.py\u001b[0m in \u001b[0;36magent_reward\u001b[0;34m(self, agent, world)\u001b[0m\n\u001b[1;32m    175\u001b[0m             adv_rew = sum(\n\u001b[1;32m    176\u001b[0m                 \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_pos\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoal_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0madversary_agents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             )\n\u001b[1;32m    179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# proximity-based adversary reward (binary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pettingzoo/mpe/simple_adversary/simple_adversary.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshaped_adv_reward\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# distance-based adversary reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             adv_rew = sum(\n\u001b[0;32m--> 176\u001b[0;31m                 \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_pos\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoal_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0madversary_agents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Good_score_history)"
      ],
      "metadata": {
        "id": "TUavDpIdULEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Opp_score_history)"
      ],
      "metadata": {
        "id": "sJSf_trXBItI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/adversary_0_actor /content/adversary_0_actor.zip\n",
        "!zip -r /content/agent_0_actor /content/agent_0_actor.zip\n",
        "!zip -r /content/agent_1_actor /content/agent_1_actor.zip"
      ],
      "metadata": {
        "id": "ls-IFXq_xfJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('/content/agent_1_actor.zip')\n",
        "files.download('/content/agent_0_actor.zip')\n",
        "files.download('/content/adversary_0_actor.zip')"
      ],
      "metadata": {
        "id": "UXUicb9d3CYO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}