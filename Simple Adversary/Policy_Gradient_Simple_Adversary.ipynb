{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9vDMUCyBNGiR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc18c191-0a63-4cf7-f0d9-2103eefb0a95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pettingzoo[mpe]\n",
            "  Downloading PettingZoo-1.22.2-py3-none-any.whl (816 kB)\n",
            "\u001b[K     |████████████████████████████████| 816 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting gymnasium>=0.26.0\n",
            "  Downloading Gymnasium-0.26.3-py3-none-any.whl (836 kB)\n",
            "\u001b[K     |████████████████████████████████| 836 kB 49.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from pettingzoo[mpe]) (1.21.6)\n",
            "Collecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.3 MB 100 kB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gymnasium>=0.26.0->pettingzoo[mpe]) (4.13.0)\n",
            "Collecting gymnasium-notices>=0.0.1\n",
            "  Downloading gymnasium_notices-0.0.1-py3-none-any.whl (2.8 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gymnasium>=0.26.0->pettingzoo[mpe]) (1.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gymnasium>=0.26.0->pettingzoo[mpe]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gymnasium>=0.26.0->pettingzoo[mpe]) (3.10.0)\n",
            "Installing collected packages: gymnasium-notices, gymnasium, pygame, pettingzoo\n",
            "Successfully installed gymnasium-0.26.3 gymnasium-notices-0.0.1 pettingzoo-1.22.2 pygame-2.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pettingzoo[mpe]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input,Dense,Activation\n",
        "from keras.models import Model,load_model\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import keras.backend as K\n",
        "\n",
        "class Agent(object):\n",
        "  def __init__ (self,name,alpha,gamma,input_dims, n_actions,layer_size,fname ='reinforcePolicy'):\n",
        "    self.gamma = gamma\n",
        "    self.lr = alpha\n",
        "    self.G = 0\n",
        "    self.input_dims = input_dims\n",
        "    self.layer_size = layer_size\n",
        "    self.n_actions = n_actions\n",
        "    self.state_memory = []\n",
        "    self.action_memory = []\n",
        "    self.reward_memory = []\n",
        "    self.agent_name = name\n",
        "\n",
        "\n",
        "    self.policy,self.predict = self.create_policy()\n",
        "    self.action_space = [i for i in range(n_actions)]\n",
        "    self.model_file = fname+' '+self.agent_name\n",
        "\n",
        "  def create_policy(self):\n",
        "    input = Input(shape = (self.input_dims,))#comma indicates that it takes a batch\n",
        "    advantages = Input(shape =[1])\n",
        "    dense1 = Dense(self.layer_size,activation = 'relu')(input)\n",
        "    dense2 = Dense(self.layer_size,activation = 'relu')(dense1)\n",
        "    probs = Dense(self.n_actions, activation = 'softmax')(dense2)\n",
        "\n",
        "    def custom_loss(y_true,y_pred):\n",
        "      out = K.clip(y_pred,1e-5, 1-1e-5) #this is to ensure that we do not perform log calcualtions with log values of 0\n",
        "      log_lik = y_true*K.log(out)\n",
        "\n",
        "      return K.sum(-log_lik*advantages)\n",
        "    \n",
        "    policy = Model(inputs = [input,advantages], outputs = [probs])\n",
        "    opt = keras.optimizers.Adam(learning_rate = self.lr)\n",
        "    policy.compile(optimizer = opt, loss = custom_loss)\n",
        "\n",
        "    predict = keras.Model(inputs = [input], outputs = [probs])\n",
        "\n",
        "    return policy,predict\n",
        "\n",
        "  def choose_action(self,obsv): \n",
        "    obsv = np.expand_dims(obsv,0)\n",
        "    #to keep the input shape consistant\n",
        "    probabilities = self.predict.predict(obsv)[0]\n",
        "    # we take the 0th element because predict returns a tuple\n",
        "    action = np.random.choice(self.action_space,p=probabilities)\n",
        "    # action = np.argmax(probabilities)\n",
        "    return action\n",
        "\n",
        "  def store_trans(self,obsv,action,reward):\n",
        "    self.action_memory.append(action)\n",
        "    self.state_memory.append(obsv)\n",
        "    self.reward_memory.append(reward)\n",
        "\n",
        "  def learn(self):\n",
        "    state_memory = np.array(self.state_memory)\n",
        "    reward_memory = np.array(self.reward_memory)\n",
        "    action_memory = np.array(self.action_memory)\n",
        "\n",
        "\n",
        "    actions = np.zeros([len(action_memory),self.n_actions])\n",
        "    actions[np.arange(len(action_memory)),action_memory] = 1\n",
        "    # loss function requires the labels to be 1 hot encoded\n",
        "\n",
        "    G = np.zeros_like(reward_memory)\n",
        "    for t in range(len(reward_memory)):\n",
        "      G_sum = 0\n",
        "      discount = 1\n",
        "      for k in range(t,len(reward_memory)):\n",
        "        G_sum += reward_memory[k]*discount\n",
        "        discount *= self.gamma\n",
        "      G[t] = G_sum\n",
        "\n",
        "    mean = np.mean(G)\n",
        "    std = np.std(G) if np.std(G) > 0 else 1\n",
        "    self.G = (G-mean)/std\n",
        "    print(len(self.G))\n",
        "    print(len(state_memory))\n",
        "    print(len(actions))\n",
        "    cost = self.policy.train_on_batch([state_memory,self.G], actions) # actions is our labels\n",
        "    #[state_memory,self.G] =  y_pred and actions = y_true\n",
        "    # This is to calculate for the loss function\n",
        "\n",
        "    self.state_memory = []\n",
        "    self.reward_memory = []\n",
        "    self.action_memory = []\n",
        "\n",
        "  def save_model(self):\n",
        "    self.policy.save_weights(self.model_file)\n",
        "    self.policy.save(self.model_file)\n",
        "\n",
        "  def load_model(self):\n",
        "    self.policy.load_weights(self.model_file)"
      ],
      "metadata": {
        "id": "xcnAsMKvO6GZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "\n",
        "disable_eager_execution()"
      ],
      "metadata": {
        "id": "3loktP3BvcOM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pettingzoo.mpe import simple_adversary_v2\n",
        "env = simple_adversary_v2.env(N=2,max_cycles = 25 , continuous_actions = False)\n",
        "env.reset()"
      ],
      "metadata": {
        "id": "dV_7gQsKOfAT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_net = {}\n",
        "agent_list = []\n",
        "for a in env.agents:\n",
        "  obs_space = env.observation_space(a).shape\n",
        "  print(obs_space[0])\n",
        "  agent = Agent(a, 1e-5 , 0.99 , obs_space[0] ,5,256)\n",
        "  agent_net[a] = agent\n",
        "  agent_list.append(a)"
      ],
      "metadata": {
        "id": "1uC-vZWhh2Uf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fedb82c5-2144-493c-e214-1f9b3ab50805"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n",
            "10\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()\n",
        "max_episode = 1000\n",
        "good_best_score = -1000\n",
        "opp_best_score = -10000\n",
        "Good_score_history = []\n",
        "Opp_score_history = []\n",
        "good_score = 0\n",
        "opp_score  = 0\n",
        "Good_avg_score = []\n",
        "Opp_score_history = []\n",
        "\n",
        "for i in range(max_episode):\n",
        "  opp_score = 0\n",
        "  good_score = 0\n",
        "  env.reset()\n",
        "  for agent in env.agent_iter():\n",
        "    observation,reward,done,trunc,_= env.last()\n",
        "    if (agent == 'adversary_0'):\n",
        "      opp_score += reward\n",
        "    else:\n",
        "      good_score += reward \n",
        "    if not (done or trunc):\n",
        "      action = agent_net[agent].choose_action(observation)\n",
        "      env.step(action)\n",
        "      agent_net[agent].store_trans(observation,action,reward)\n",
        "    else:\n",
        "      action = None\n",
        "      env.step(action)\n",
        "\n",
        "  Good_score_history.append(good_score)\n",
        "  Opp_score_history.append(opp_score)\n",
        "\n",
        "  Good_avg_score = np.mean(Good_score_history[-100:])\n",
        "  Opp_avg_score = np.mean(Opp_score_history[-100:])\n",
        "\n",
        "  for agent in env.agents:\n",
        "    agent_net[agent].learn()\n",
        "  \n",
        "  if Good_avg_score > good_best_score:\n",
        "    load_checkpoint = True\n",
        "    good_best_score = Good_avg_score\n",
        "    for a in agent_list:\n",
        "      if (a != 'adversary_0'):\n",
        "        agent_net[a].save_model()\n",
        "\n",
        "  if Opp_avg_score > opp_best_score:\n",
        "    load_checkpoint = True\n",
        "    opp_best_score = Opp_avg_score\n",
        "    for a in agent_list:\n",
        "      if (a == 'adversary_0'):\n",
        "        agent_net[a].save_model()\n",
        " \n",
        "\n",
        "\n",
        "  print('episode: ', i ,' Opp avg score:', Opp_avg_score,'Good avg score :',Good_avg_score)"
      ],
      "metadata": {
        "id": "cHQlUqmaosSm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78f6d5fe-39c8-4469-8e75-5db68004e1c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode:  0  Opp avg score: -17.73136877892052 Good avg score : -15.820822571087099\n",
            "episode:  1  Opp avg score: -14.774178950045151 Good avg score : -8.43626675544986\n",
            "episode:  2  Opp avg score: -26.3690510889497 Good avg score : 12.91410715224606\n",
            "episode:  3  Opp avg score: -23.887606908238222 Good avg score : 1.6586724781292492\n",
            "episode:  4  Opp avg score: -26.8490767426516 Good avg score : 2.1097004981065615\n",
            "episode:  5  Opp avg score: -25.453988725890284 Good avg score : -1.2198243166113194\n",
            "episode:  6  Opp avg score: -27.190126388350986 Good avg score : 0.5847595192566709\n",
            "episode:  7  Opp avg score: -27.73157435512867 Good avg score : 3.239515933040388\n",
            "episode:  8  Opp avg score: -27.679839220404162 Good avg score : 7.185847056682885\n",
            "episode:  9  Opp avg score: -26.73341537010581 Good avg score : 6.885403819832108\n",
            "episode:  10  Opp avg score: -26.023473550502942 Good avg score : 3.868634055201302\n",
            "episode:  11  Opp avg score: -28.113352845058596 Good avg score : 6.575467109333112\n",
            "episode:  12  Opp avg score: -28.536361202858984 Good avg score : 9.974991132189254\n",
            "episode:  13  Opp avg score: -27.457890300978057 Good avg score : 9.651273338623225\n",
            "episode:  14  Opp avg score: -26.93064341251901 Good avg score : 6.453903152432141\n",
            "episode:  15  Opp avg score: -27.941479089097733 Good avg score : 10.348236479485937\n",
            "episode:  16  Opp avg score: -28.03143175907995 Good avg score : 9.422945288166577\n",
            "episode:  17  Opp avg score: -28.112297667243197 Good avg score : 8.892123743785048\n",
            "episode:  18  Opp avg score: -28.274726153140147 Good avg score : 9.984125445412955\n",
            "episode:  19  Opp avg score: -28.488113528586986 Good avg score : 9.824611884252302\n",
            "episode:  20  Opp avg score: -28.920803743515847 Good avg score : 11.20373183740435\n",
            "episode:  21  Opp avg score: -28.93905341345604 Good avg score : 10.908552991219407\n",
            "episode:  22  Opp avg score: -29.239173648597934 Good avg score : 11.898259297491071\n",
            "episode:  23  Opp avg score: -29.259372400085997 Good avg score : 12.83109432961149\n",
            "episode:  24  Opp avg score: -29.474840078822456 Good avg score : 12.329777942910988\n",
            "episode:  25  Opp avg score: -29.976304150728065 Good avg score : 13.341218746889199\n",
            "episode:  26  Opp avg score: -29.828140364302886 Good avg score : 14.000975644575975\n",
            "episode:  27  Opp avg score: -30.592147257800182 Good avg score : 16.070448602672467\n",
            "episode:  28  Opp avg score: -30.85325471442349 Good avg score : 16.429948659129227\n",
            "episode:  29  Opp avg score: -30.184299063406588 Good avg score : 16.209214216817557\n",
            "episode:  30  Opp avg score: -30.253775448510364 Good avg score : 16.047779245892738\n",
            "episode:  31  Opp avg score: -29.933129928606107 Good avg score : 16.00760651219318\n",
            "episode:  32  Opp avg score: -29.937893333211633 Good avg score : 15.826022642779066\n",
            "episode:  33  Opp avg score: -30.089619108730066 Good avg score : 15.54011311235525\n",
            "episode:  34  Opp avg score: -29.692399334394075 Good avg score : 14.872052724029459\n",
            "episode:  35  Opp avg score: -29.347869536239003 Good avg score : 14.551881239781153\n",
            "episode:  36  Opp avg score: -29.084008773397525 Good avg score : 14.088935239771272\n",
            "episode:  37  Opp avg score: -28.83833655768572 Good avg score : 14.031339492507126\n",
            "episode:  38  Opp avg score: -28.719558117888216 Good avg score : 14.07838470798219\n",
            "episode:  39  Opp avg score: -28.28705960051631 Good avg score : 12.364913795933152\n",
            "episode:  40  Opp avg score: -28.393994148187474 Good avg score : 12.793527648207126\n",
            "episode:  41  Opp avg score: -28.48724331439522 Good avg score : 12.151262837164698\n",
            "episode:  42  Opp avg score: -28.958290106820794 Good avg score : 13.652274385061663\n",
            "episode:  43  Opp avg score: -29.421578147829813 Good avg score : 14.43733866768464\n",
            "episode:  44  Opp avg score: -29.606028160435976 Good avg score : 14.496786469802228\n",
            "episode:  45  Opp avg score: -29.847105820668187 Good avg score : 14.939343013179165\n",
            "episode:  46  Opp avg score: -29.310428112186457 Good avg score : 13.677814798573603\n",
            "episode:  47  Opp avg score: -29.28881528512939 Good avg score : 14.050929251201618\n",
            "episode:  48  Opp avg score: -29.41601559277506 Good avg score : 14.91378407917569\n",
            "episode:  49  Opp avg score: -29.740010757288438 Good avg score : 15.930866129944206\n",
            "episode:  50  Opp avg score: -29.45644373349981 Good avg score : 14.502336474326048\n",
            "episode:  51  Opp avg score: -29.058562897580337 Good avg score : 13.795740483312528\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Good_score_history)"
      ],
      "metadata": {
        "id": "TUavDpIdULEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Opp_score_history)"
      ],
      "metadata": {
        "id": "sJSf_trXBItI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/adversary_0_actor /content/adversary_0_actor.zip\n",
        "!zip -r /content/agent_0_actor /content/agent_0_actor.zip\n",
        "!zip -r /content/agent_1_actor /content/agent_1_actor.zip"
      ],
      "metadata": {
        "id": "ls-IFXq_xfJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('/content/agent_1_actor.zip')\n",
        "files.download('/content/agent_0_actor.zip')\n",
        "files.download('/content/adversary_0_actor.zip')"
      ],
      "metadata": {
        "id": "UXUicb9d3CYO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}