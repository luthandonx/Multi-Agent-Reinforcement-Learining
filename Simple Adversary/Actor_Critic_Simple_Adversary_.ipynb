{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BoeWWpffPWV",
        "outputId": "121a22f5-aedd-4aa9-9083-8d311d396c13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pettingzoo[mpe]\n",
            "  Downloading PettingZoo-1.22.2-py3-none-any.whl (816 kB)\n",
            "\u001b[K     |████████████████████████████████| 816 kB 35.2 MB/s \n",
            "\u001b[?25hCollecting gymnasium>=0.26.0\n",
            "  Downloading Gymnasium-0.26.3-py3-none-any.whl (836 kB)\n",
            "\u001b[K     |████████████████████████████████| 836 kB 56.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from pettingzoo[mpe]) (1.21.6)\n",
            "Collecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.3 MB 138 kB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gymnasium>=0.26.0->pettingzoo[mpe]) (4.13.0)\n",
            "Collecting gymnasium-notices>=0.0.1\n",
            "  Downloading gymnasium_notices-0.0.1-py3-none-any.whl (2.8 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gymnasium>=0.26.0->pettingzoo[mpe]) (1.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gymnasium>=0.26.0->pettingzoo[mpe]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gymnasium>=0.26.0->pettingzoo[mpe]) (3.10.0)\n",
            "Installing collected packages: gymnasium-notices, gymnasium, pygame, pettingzoo\n",
            "Successfully installed gymnasium-0.26.3 gymnasium-notices-0.0.1 pettingzoo-1.22.2 pygame-2.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pettingzoo[mpe]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "x2pYlVprPd2l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model,layers\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "class ActorCriticNetwork(Model):\n",
        "  def __init__(self,n_actions, dims_1 = 256 , dims_2 = 256,\n",
        "               name = 'actor_critc', chkpt_dir = 'tmp/actor_critic'):\n",
        "    super(ActorCriticNetwork, self).__init__()\n",
        "    self.dims_1 = dims_1\n",
        "    self.dims_2 = dims_2\n",
        "    self.n_actions = n_actions\n",
        "    self.model_name = name\n",
        "    self.checkpoint_dir = chkpt_dir\n",
        "    self.checkpoint_file = os.path.join(self.checkpoint_dir, name+'_ac')\n",
        "\n",
        "    self.fc1 = layers.Dense(self.dims_1,activation = 'relu')\n",
        "    self.fc2 = layers.Dense(self.dims_2,activation = 'relu')\n",
        "    self.v = layers.Dense(1,activation = None)\n",
        "    self.po = layers.Dense(n_actions,activation = 'softmax')\n",
        "\n",
        "  def call(self,state):\n",
        "    value = self.fc1(state)\n",
        "    value = self.fc2(value)\n",
        "\n",
        "    v = self.v(value)\n",
        "    po = self.po(value)\n",
        "\n",
        "    return v,po"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UjVVvJMuTH2b"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "class Agent:\n",
        "  def __init__(self, alpha = 1e-5, gamma = 0.95, n_actions = 4,name='agent'):\n",
        "    self.gamma = gamma\n",
        "    self.n_actions = n_actions\n",
        "    self.action = None\n",
        "    self.action_space = [i for i in range(self.n_actions)]\n",
        "    self.name = name\n",
        "    self.actor_critic = ActorCriticNetwork(n_actions=n_actions)\n",
        "    self.actor_critic.compile(optimizer = Adam(learning_rate = alpha))\n",
        "    self.fname = 'actor_critic_'+self.name\n",
        "\n",
        "    self.reward_history = []\n",
        "    self.action_history = []\n",
        "    self.obsv_history = []\n",
        "    self.next_obsv_history = []\n",
        "\n",
        "  def choose_action(self,observation):\n",
        "    state = tf.convert_to_tensor([observation])\n",
        "    _,probs = self.actor_critic(state)\n",
        "\n",
        "    action_probs = tfp.distributions.Categorical(probs = probs)\n",
        "    action = action_probs.sample()\n",
        "\n",
        "    self.action = action\n",
        "\n",
        "    return action.numpy()[0]\n",
        "\n",
        "  def save_models(self):\n",
        "    print('saving model.....')\n",
        "    self.actor_critic.save(self.fname)\n",
        "  \n",
        "  def load_models(self):\n",
        "    #print('loading model....')\n",
        "    self.actor_critic = tf.keras.models_load_model(self.fname)\n",
        "\n",
        "  def store_trans(self,action,reward,obsv,obsv_):\n",
        "      self.action_history.append(action)\n",
        "      self.reward_history.append(reward)\n",
        "      self.obsv_history.append(obsv)\n",
        "      self.next_obsv_history.append(obsv_)\n",
        "\n",
        "  def clear_memory(self):\n",
        "    self.reward_history = []\n",
        "    self.action_history = []\n",
        "    self.obsv_history = []\n",
        "    self.next_obsv_history = []\n",
        "\n",
        "  def learn(self,state,reward,state_,done):\n",
        "    state = tf.convert_to_tensor([state],dtype = tf.float32)\n",
        "    state_ = tf.convert_to_tensor([state_],dtype = tf.float32)\n",
        "    reward = tf.convert_to_tensor(reward,dtype = tf.float32)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      state_value, probs = self.actor_critic(state)\n",
        "      state_value_,_ = self.actor_critic(state_)\n",
        "      state_value = tf.squeeze(state_value)\n",
        "      state_value_ = tf.squeeze(state_value_)\n",
        "\n",
        "      action_probs = tfp.distributions.Categorical(probs = probs)\n",
        "      log_probs = action_probs.log_prob(self.action)\n",
        "\n",
        "      delta = reward + self.gamma*state_value_*(1-int(done)) - state_value\n",
        "\n",
        "      actor_loss = -log_probs*delta\n",
        "      critic_loss = delta**2\n",
        "\n",
        "      total_loss = actor_loss + critic_loss\n",
        "\n",
        "    gradients = tape.gradient(total_loss,self.actor_critic.trainable_variables)\n",
        "    self.actor_critic.optimizer.apply_gradients(zip(gradients, self.actor_critic.trainable_variables))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JjZt3bDBfG2M"
      },
      "outputs": [],
      "source": [
        "from pettingzoo.mpe import simple_adversary_v2\n",
        "import numpy as np\n",
        "\n",
        "env = simple_adversary_v2.env(N=2,max_cycles = 25 , continuous_actions = False )\n",
        "env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kS-tdY_mf9SG"
      },
      "outputs": [],
      "source": [
        "env.reset()\n",
        "agent_net = {}\n",
        "agent_list = []\n",
        "for a in env.agents:\n",
        "  n_actions = env.action_space(a).n\n",
        "  input_dims = env.observation_space(a).shape[0]\n",
        "  if (a == 'adversary_0'):\n",
        "    opp_agent = Agent(alpha = 1e-5,n_actions = 5,name = a)\n",
        "  else:\n",
        "    good_agent = Agent(alpha = 1e-5,n_actions = 5,name = a)\n",
        "    agent_net[a] = good_agent\n",
        "    agent_list.append(a)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fAngBQwELWlc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-GfIVgMQgrZe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "outputId": "2f9a2aec-a4cf-4e8e-e307-793495d26b62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving model.....\n",
            "saving model.....\n",
            "saving model.....\n",
            "episode:  0  Opp avg score: -28.16110626373537 Good avg score : 16.706423700464363\n",
            "saving model.....\n",
            "saving model.....\n",
            "episode:  1  Opp avg score: -41.89718006406124 Good avg score : 47.791389812068076\n",
            "episode:  2  Opp avg score: -42.350185130371734 Good avg score : 36.77731602358845\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-8775198cc0b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'adversary_0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopp_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobsv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mopp_score\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-926e564d0344>\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-291>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logits, probs, dtype, validate_args, allow_nan_stats, name)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/distributions/distribution.py\u001b[0m in \u001b[0;36mwrapped_init\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    340\u001b[0m       \u001b[0;31m# called, here is the place to do it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m       \u001b[0mself_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m       \u001b[0mdefault_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m       \u001b[0;31m# Note: if we ever want to override things set in `self` by subclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m       \u001b[0;31m# `__init__`, here is the place to do it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/distributions/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logits, probs, dtype, validate_args, allow_nan_stats, name)\u001b[0m\n\u001b[1;32m    199\u001b[0m           \u001b[0mallow_nan_stats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_nan_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m           \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/distributions/distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dtype, reparameterization_type, validate_args, allow_nan_stats, parameters, graph_parents, name)\u001b[0m\n\u001b[1;32m    614\u001b[0m             else name)\n\u001b[1;32m    615\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip_invalid_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_name_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstructor_name_scope\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/module/module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscope_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/tf2.py\u001b[0m in \u001b[0;36menabled\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0menabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;31m# Returns True iff TensorFlow 2.0 behavior should be enabled.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_pywrap_tf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "env.reset()\n",
        "max_episode = 1000\n",
        "good_best_score = -1000\n",
        "opp_best_score = -10000\n",
        "Good_score_history = []\n",
        "Opp_score_history = []\n",
        "good_score = 0\n",
        "opp_score  = 0\n",
        "Opp_state_ = []\n",
        "Good_state_ = []\n",
        "Good_avg_score = []\n",
        "Opp_score_history = []\n",
        "n_games = 1000\n",
        "\n",
        "for i in range(n_games):\n",
        "  env.reset()\n",
        "  good_score = 0\n",
        "  opp_score  = 0\n",
        "  for agent in env.agent_iter():\n",
        "    obsv,reward,done,trunc,_ = env.last()\n",
        "    if not (done or trunc):\n",
        "      if (agent == 'adversary_0'):\n",
        "        action = opp_agent.choose_action(obsv)\n",
        "        opp_score += reward\n",
        "        env.step(action)\n",
        "        obsv_,_,_,_,_ = env.last()\n",
        "        if(len(Opp_state_)>0):\n",
        "          opp_agent.store_trans(action,reward,obsv,Opp_state_[0])\n",
        "          opp_agent.learn(obsv,reward,Opp_state_[0],done)\n",
        "      else:\n",
        "        action = agent_net[agent].choose_action(obsv)\n",
        "        good_score += reward\n",
        "        env.step(action)\n",
        "        state_,_,_,_,_ = env.last()\n",
        "        if (state_.shape == obsv.shape):\n",
        "          agent_net[agent].store_trans(action,reward,obsv,state_)\n",
        "          agent_net[agent].learn(obsv,reward,state_,done)\n",
        "        else:\n",
        "          Opp_state_.append(state_)  \n",
        "    else:\n",
        "      env.step(None)   \n",
        "  \n",
        "  Good_score_history.append(good_score)\n",
        "  Opp_score_history.append(opp_score)\n",
        "\n",
        "  Good_avg_score = np.mean(Good_score_history[-100:])\n",
        "  Opp_avg_score = np.mean(Opp_score_history[-100:])\n",
        "\n",
        "  if (Good_avg_score > good_best_score):\n",
        "    good_best_score = Good_avg_score\n",
        "    for a in agent_list:\n",
        "       agent_net[a].save_models()\n",
        "\n",
        "  if (Opp_avg_score > opp_best_score):\n",
        "    opp_best_score = Opp_avg_score\n",
        "    opp_agent.save_models()\n",
        "  \n",
        "  print('episode: ', i ,' Opp avg score:', Opp_avg_score,'Good avg score :',Good_avg_score)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/actor_critic_adversary_0.zip /content/actor_critic_adversary_0\n",
        "!zip -r /content/actor_critic_agent_1.zip /content/actor_critic_agent_1\n",
        "!zip -r /content/actor_critic_agent_0.zip /content/actor_critic_agent_0"
      ],
      "metadata": {
        "id": "q_L90rSkfW25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n"
      ],
      "metadata": {
        "id": "EekTuwoTf9iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !zip -r /content/actor_critic_agent_0.zip /content/actor_critic_agent_0\n",
        "# !zip -r /content/actor_critic_agent_1.zip /content/actor_critic_agent_1\n",
        "files.download('/content/actor_critic_adversary_0.zip')\n",
        "files.download('/content/actor_critic_agent_0.zip')\n",
        "files.download('/content/actor_critic_agent_1.zip')"
      ],
      "metadata": {
        "id": "uXYbwxr6kb6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4q7w_pkNZQX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import savgol_filter\n",
        "\n",
        "x = np.linspace(0,2*np.pi,100)\n",
        "y = np.sin(x) + np.random.random(100) * 0.2\n",
        "yhat = savgol_filter(Good_score_history, 101, 7) # window size 51, polynomial order 3\n",
        "opphat = savgol_filter(Opp_score_history, 101, 7) \n",
        "\n",
        "plt.plot(yhat, color='red')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THSjSQ9jRorP"
      },
      "outputs": [],
      "source": [
        "print(Good_score_history)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Opp_score_history)"
      ],
      "metadata": {
        "id": "LlFGbuxpPxt2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}