{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHe6t4r1pJHX",
        "outputId": "266783d6-a0c9-4dbb-c3f0-bf0ea4be1d7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pettingzoo[mpe]\n",
            "  Downloading PettingZoo-1.22.2-py3-none-any.whl (816 kB)\n",
            "\u001b[K     |████████████████████████████████| 816 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from pettingzoo[mpe]) (1.21.6)\n",
            "Collecting gymnasium>=0.26.0\n",
            "  Downloading Gymnasium-0.26.3-py3-none-any.whl (836 kB)\n",
            "\u001b[K     |████████████████████████████████| 836 kB 42.1 MB/s \n",
            "\u001b[?25hCollecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.3 MB 42.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gymnasium>=0.26.0->pettingzoo[mpe]) (1.5.0)\n",
            "Collecting gymnasium-notices>=0.0.1\n",
            "  Downloading gymnasium_notices-0.0.1-py3-none-any.whl (2.8 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gymnasium>=0.26.0->pettingzoo[mpe]) (4.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gymnasium>=0.26.0->pettingzoo[mpe]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gymnasium>=0.26.0->pettingzoo[mpe]) (3.10.0)\n",
            "Installing collected packages: gymnasium-notices, gymnasium, pygame, pettingzoo\n",
            "Successfully installed gymnasium-0.26.3 gymnasium-notices-0.0.1 pettingzoo-1.22.2 pygame-2.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pettingzoo[mpe]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qQdNuZacotZ1"
      },
      "outputs": [],
      "source": [
        "from pettingzoo.mpe import simple_adversary_v2\n",
        "env = simple_adversary_v2.env(N=2,max_cycles = 25 , continuous_actions = False)\n",
        "env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NVuwU7oEsU7T"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Replay():\n",
        "  def __init__(self,max_size,n_actions,input_shape):\n",
        "    self.mem_size = max_size\n",
        "    self.mem_counter = 0\n",
        "    self.state_memory = np.zeros((self.mem_size,input_shape))\n",
        "    self.new_state_memory = np.zeros((self.mem_size,input_shape))\n",
        "    self.action_memory = np.zeros((self.mem_size,n_actions))\n",
        "    self.reward_memory = np.zeros(self.mem_size)\n",
        "    self.terminal_memory = np.zeros(self.mem_size,dtype = bool)\n",
        "\n",
        "  def store_trans(self,state,action,reward,done):\n",
        "    index = self.mem_counter % self.mem_size\n",
        "    self.state_memory[index] = state\n",
        "    self.action_memory[index] = action\n",
        "    self.reward_memory[index] = reward\n",
        "   \n",
        "    self.terminal_memory[index] = done\n",
        "    if (self.mem_counter > 0):\n",
        "       self.new_state_memory[index-1] = state\n",
        "\n",
        "    self.mem_counter += 1\n",
        "\n",
        "  def sample_buffer(self,batch):\n",
        "    self.batch = batch\n",
        "    states = self.state_memory[batch]\n",
        "    actions = self.action_memory[batch]\n",
        "    rewards = self.reward_memory[batch]\n",
        "    states_ = self.new_state_memory[batch]\n",
        "    dones = self.terminal_memory[batch]\n",
        "    \n",
        "    return   states,actions,rewards,states_,dones\n",
        "\n",
        "  def returnSample(self,batch_size):\n",
        "    max_mem = min(batch_size,self.mem_counter)\n",
        "    batch = np.random.choice(max_mem,batch_size) \n",
        "    return batch\n",
        "\n",
        "  def returnshit(self):\n",
        "    return self.state_memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "agL75qrmXseD"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "class CriticNet(keras.Model):\n",
        "  def __init__(self,n_actions, dim_layer):\n",
        "    super(CriticNet,self).__init__()\n",
        "    self.dim_size = dim_layer\n",
        "    self.n_actions = n_actions\n",
        "    self.dim_size = 128\n",
        "    self.layer_one = Dense(self.dim_size,activation='relu')\n",
        "    self.layer_two = Dense(self.dim_size,activation='relu')\n",
        "    self.q_value = Dense(1,activation=None)\n",
        "\n",
        "  def call(self,inputs):\n",
        "    states,actions = inputs\n",
        "    input_thing = tf.concat([states, actions],axis = 1)\n",
        "    action_value = self.layer_one(input_thing)\n",
        "    action_value = self.layer_two(action_value)\n",
        "\n",
        "    q = self.q_value(action_value)\n",
        "\n",
        "    return q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DfQnMLftdaZg"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "class ActorNet(keras.Model):\n",
        "  def __init__(self,n_actions, dim_layer):\n",
        "    super(ActorNet,self).__init__()\n",
        "    self.dim_size = dim_layer\n",
        "    self.n_actions = n_actions\n",
        "\n",
        "    self.layer_one = Dense(self.dim_size, activation = 'relu')\n",
        "    self.layer_two = Dense(self.dim_size, activation = 'relu')\n",
        "    self.policy = Dense(self.n_actions, activation = 'softmax')\n",
        "\n",
        "  def call(self,state):\n",
        "    probs = self.layer_one(state)\n",
        "    probs = self.layer_two(probs)\n",
        "\n",
        "    mu = self.policy(probs)\n",
        "    return mu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent_Good:\n",
        "  def __init__(self,input_dims,alpha = 0.01,beta =1e-5 , env = None ,\n",
        "               gamma = 0.95, n_actions = 5, max_size = 10000, tau = 0.001,\n",
        "               layer_size = 256, batch_size = 50,noise = 0.1, name = None):\n",
        "    \n",
        "    self.gamma = gamma\n",
        "    self.tau = tau\n",
        "    self.memory = Replay(max_size,n_actions,input_dims)\n",
        "    self.batch_size = batch_size\n",
        "    self.n_actions = n_actions\n",
        "    self.act_name = name+' actor'\n",
        "    self.cri_name = name+' critc'\n",
        "    self.tar_act_name = name+' target_actor'\n",
        "    self.tar_cri_name = name+' target_critc'\n",
        "    \n",
        "    self.actor = ActorNet(n_actions = n_actions, dim_layer = layer_size)\n",
        "    self.critic = CriticNet(n_actions,layer_size)\n",
        "\n",
        "    self.target_actor = ActorNet(n_actions = n_actions, dim_layer = layer_size)\n",
        "    self.target_critic = CriticNet(n_actions,layer_size)\n",
        "\n",
        "    self.actor.compile(keras.optimizers.Adam(learning_rate=alpha))\n",
        "    self.target_actor.compile(keras.optimizers.Adam(learning_rate=alpha))\n",
        "\n",
        "    self.critic.compile(keras.optimizers.Adam(learning_rate=beta))\n",
        "    self.target_critic.compile(keras.optimizers.Adam(learning_rate=beta))\n",
        "\n",
        "    self.learnCounter = 0\n",
        "\n",
        "  def Update_net_parameters(self,tau=None):\n",
        "    if tau is None:\n",
        "      tau = self.tau\n",
        "    \n",
        "    weights = []\n",
        "    targets = self.target_actor.weights\n",
        "\n",
        "    for i,weight in enumerate(self.actor.weights):\n",
        "      weights.append(weight * tau + targets[i] *(1-tau))\n",
        "    self.target_actor.set_weights(weights)\n",
        "\n",
        "    weights = []\n",
        "    targets = self.target_critic.weights  \n",
        "    for i,weight in enumerate(self.critic.weights):\n",
        "      weights.append(weight * tau + targets[i] *(1-tau))\n",
        "    self.target_critic.set_weights(weights)\n",
        "\n",
        "  def store_transition(self,state,action,reward,done):\n",
        "    self.memory.store_trans(state,action,reward,done)\n",
        "\n",
        "  def save_models(self):\n",
        "    print('....saving models....')\n",
        "    self.actor.save_weights(self.act_name)\n",
        "    self.critic.save_weights(self.cri_name)\n",
        "    self.target_actor.save_weights(self.tar_act_name)\n",
        "    self.target_critic.save_weights(self.tar_cri_name)\n",
        "\n",
        "    self.actor.save(self.act_name)\n",
        "\n",
        "  def load_models(self):\n",
        "    print('....loading models....')\n",
        "    self.actor.load_weights(self.act_name)\n",
        "    self.critic.load_weights(self.cri_name)\n",
        "    self.target_actor.load_weights(self.tar_act_name)\n",
        "    self.target_critic.load_weights(self.tar_cri_name)\n",
        "\n",
        "  def choose_action(self,observation,evaluate = False):\n",
        "    state = tf.convert_to_tensor([observation])\n",
        "    probs = self.actor(state)\n",
        "    action_probs = tfp.distributions.Categorical(probs = probs)\n",
        "    action = action_probs.sample()\n",
        "    action = action.numpy()[0]\n",
        "    return action\n",
        "\n",
        "  def learn(self,agents):\n",
        "    \n",
        "    if self.memory.mem_counter < self.batch_size:\n",
        "      return\n",
        "    \n",
        "    self.learnCounter += 1\n",
        "    agent_2 = agents[0]\n",
        "\n",
        "    samples = self.memory.returnSample(self.batch_size)\n",
        "\n",
        "    state_1,action_1,reward_1,new_state_1,done_1 = self.memory.sample_buffer(samples)\n",
        "    states_1 = tf.convert_to_tensor(state_1)\n",
        "    new_states_1 = tf.convert_to_tensor(new_state_1)\n",
        "    rewards_1 = tf.convert_to_tensor(reward_1, dtype= tf.float32)\n",
        "    actions_1 = tf.convert_to_tensor(action_1,dtype= tf.float32)\n",
        "\n",
        "    state_2,action_2,reward_2,new_state_2,done_2 = agent_2.memory.sample_buffer(samples)\n",
        "    states_2 = tf.convert_to_tensor(state_2)\n",
        "    new_states_2 = tf.convert_to_tensor(new_state_2)\n",
        "    rewards_2 = tf.convert_to_tensor(reward_2)\n",
        "    actions_2 = tf.convert_to_tensor(action_2,dtype= tf.float32)\n",
        "\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        target_actions_probs_1 = self.target_actor(new_states_1)\n",
        "        target_probs = tfp.distributions.Categorical(probs = target_actions_probs_1)\n",
        "        target_actions_1 = tf.transpose([target_probs.sample()])\n",
        "    \n",
        "        target_actions_probs_2 = agent_2.target_actor(new_states_2)\n",
        "        target_probs = tfp.distributions.Categorical(probs = target_actions_probs_2)\n",
        "        target_actions_2 = tf.transpose([target_probs.sample()])\n",
        "      \n",
        "\n",
        "        actions_1 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(actions_1), [0]))\n",
        "        actions_2 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(actions_2), [0]))\n",
        "        actions = tf.concat([actions_1,actions_2],axis = 1)\n",
        "        actions = tf.cast(actions,tf.float32)\n",
        "\n",
        "        new_states = tf.concat([new_states_1,new_states_2],axis = 1)\n",
        "        target_actions = tf.concat([target_actions_1,target_actions_2], axis = 1)\n",
        "        target_actions = tf.cast(target_actions,tf.float32)\n",
        "\n",
        "      \n",
        "        critic_value_ = tf.squeeze(self.target_critic((new_states,target_actions)),1)\n",
        "        states= tf.concat([states_1,states_2],axis = 1)\n",
        "     \n",
        "        critic_value = tf.squeeze(self.critic((states,actions)),1)\n",
        "        target = rewards_1 + self.gamma*critic_value_*(1-done_1)\n",
        "        critic_loss = keras.losses.MSE(target,critic_value)\n",
        "    \n",
        "    params = self.target_critic.trainable_variables\n",
        "    grads = tape.gradient(critic_loss,params)\n",
        "    self.critic.optimizer.apply_gradients(zip(grads,params))\n",
        "\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        new_policy_actions_probs = self.actor(states_1)\n",
        "        new_probs = tfp.distributions.Categorical(probs = new_policy_actions_probs)\n",
        "        new_actions_1 = tf.transpose([new_probs.sample()])\n",
        "\n",
        "        new_actions_probs_2 = agent_2.actor(states_2)\n",
        "        new_probs = tfp.distributions.Categorical(probs = new_actions_probs_2)\n",
        "        new_actions_2 = tf.transpose([new_probs.sample()])\n",
        "\n",
        "        new_actions = tf.concat([new_actions_1,new_actions_2],axis =1 )\n",
        "        new_actions = tf.cast(new_actions,tf.float32)\n",
        "        states = tf.concat([states_1,states_2],axis = 1)\n",
        "          \n",
        "        actor_loss = -self.critic((states,new_actions))\n",
        "        actor_loss = actor_loss[0]\n",
        "    \n",
        "    params = self.critic.trainable_variables\n",
        "    grads = tape.gradient(actor_loss,params)\n",
        "    self.actor.optimizer.apply_gradients(zip(grads,params))\n",
        "\n",
        "    if (self.learnCounter % 100 == 0):\n",
        "      self.Update_net_parameters()\n",
        "      self.learnCounter = 0"
      ],
      "metadata": {
        "id": "tjADj83U9BOk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "seJqIe7pf8EA"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "class Agent_Opp:\n",
        "  def __init__(self,input_dims,alpha = 0.01,beta =1e-5 , env = None ,\n",
        "               gamma = 0.95, n_actions = 5, max_size = 10000, tau = 0.001,\n",
        "               layer_size = 256, batch_size = 50,noise = 0.1, name = None):\n",
        "    \n",
        "    self.gamma = gamma\n",
        "    self.tau = tau\n",
        "    self.memory = Replay(max_size,n_actions,input_dims)\n",
        "    self.batch_size = batch_size\n",
        "    self.n_actions = n_actions\n",
        "    self.act_name = name+' actor'\n",
        "    self.cri_name = name+' critc'\n",
        "    self.tar_act_name = name+' target_actor'\n",
        "    self.tar_cri_name = name+' target_critc'\n",
        "    \n",
        "    self.actor = ActorNet(n_actions = n_actions, dim_layer = layer_size)\n",
        "    self.critic = CriticNet(n_actions,layer_size)\n",
        "\n",
        "    self.target_actor = ActorNet(n_actions = n_actions, dim_layer = layer_size)\n",
        "    self.target_critic = CriticNet(n_actions,layer_size)\n",
        "\n",
        "    self.actor.compile(keras.optimizers.Adam(learning_rate=alpha))\n",
        "    self.target_actor.compile(keras.optimizers.Adam(learning_rate=alpha))\n",
        "\n",
        "    self.critic.compile(keras.optimizers.Adam(learning_rate=beta))\n",
        "    self.target_critic.compile(keras.optimizers.Adam(learning_rate=beta))\n",
        "\n",
        "    self.learnCounter = 0\n",
        "\n",
        "  def Update_net_parameters(self,tau=None):\n",
        "    if tau is None:\n",
        "      tau = self.tau\n",
        "    \n",
        "    weights = []\n",
        "    targets = self.target_actor.weights\n",
        "\n",
        "    for i,weight in enumerate(self.actor.weights):\n",
        "      weights.append(weight * tau + targets[i] *(1-tau))\n",
        "    self.target_actor.set_weights(weights)\n",
        "\n",
        "    weights = []\n",
        "    targets = self.target_critic.weights  \n",
        "    for i,weight in enumerate(self.critic.weights):\n",
        "      weights.append(weight * tau + targets[i] *(1-tau))\n",
        "    self.target_critic.set_weights(weights)\n",
        "\n",
        "  def store_transition(self,state,action,reward,done):\n",
        "    self.memory.store_trans(state,action,reward,done)\n",
        "\n",
        "  def save_models(self):\n",
        "    print('....saving models....')\n",
        "    self.actor.save_weights(self.act_name)\n",
        "    self.critic.save_weights(self.cri_name)\n",
        "    self.target_actor.save_weights(self.tar_act_name)\n",
        "    self.target_critic.save_weights(self.tar_cri_name)\n",
        "\n",
        "    self.actor.save(self.act_name)\n",
        "\n",
        "  def load_models(self):\n",
        "    print('....loading models....')\n",
        "    self.actor.load_weights(self.act_name)\n",
        "    self.critic.load_weights(self.cri_name)\n",
        "    self.target_actor.load_weights(self.tar_act_name)\n",
        "    self.target_critic.load_weights(self.tar_cri_name)\n",
        "\n",
        "  def choose_action(self,observation,evaluate = False):\n",
        "    state = tf.convert_to_tensor([observation])\n",
        "    probs = self.actor(state)\n",
        "    action_probs = tfp.distributions.Categorical(probs = probs)\n",
        "    action = action_probs.sample()\n",
        "    action = action.numpy()[0]\n",
        "    return action\n",
        "\n",
        "  def learn(self):\n",
        "    if self.memory.mem_counter < self.batch_size:\n",
        "      return\n",
        "\n",
        "    self.learnCounter += 1\n",
        "\n",
        "    samples = self.memory.returnSample(self.batch_size)\n",
        "\n",
        "    state_1,action_1,reward_1,new_state_1,done_1 = self.memory.sample_buffer(samples)\n",
        "    states_1 = tf.convert_to_tensor(state_1)\n",
        "    new_states_1 = tf.convert_to_tensor(new_state_1)\n",
        "    rewards_1 = tf.convert_to_tensor(reward_1, dtype= tf.float32)\n",
        "    actions_1 = tf.convert_to_tensor(action_1,dtype= tf.float32)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        target_actions_probs_1 = self.target_actor(new_states_1)\n",
        "        target_probs = tfp.distributions.Categorical(probs = target_actions_probs_1)\n",
        "        target_actions_1 = tf.transpose([target_probs.sample()])  \n",
        "\n",
        "        actions_1 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(actions_1), [0]))\n",
        "        actions = tf.cast(actions_1,tf.float32)\n",
        "\n",
        "        target_actions = tf.cast(target_actions_1,tf.float32)\n",
        "        critic_value_ = tf.squeeze(self.target_critic((new_states_1,target_actions)),1)\n",
        "        states = states_1\n",
        "        critic_value = tf.squeeze(self.critic((states,actions)),1)\n",
        "        target = rewards_1 + self.gamma*critic_value_*(1-done_1)\n",
        "        critic_loss = keras.losses.MSE(target,critic_value)\n",
        "    params = self.target_critic.trainable_variables\n",
        "    grads = tape.gradient(critic_loss,params)\n",
        "    self.critic.optimizer.apply_gradients(zip(grads,params))\n",
        "\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        new_policy_actions_probs = self.actor(states_1)\n",
        "        new_probs = tfp.distributions.Categorical(probs = new_policy_actions_probs)\n",
        "        new_actions_1 = tf.transpose([new_probs.sample()])\n",
        "        new_actions = tf.cast(new_actions_1,tf.float32)\n",
        "        states = states_1\n",
        "        actor_loss = -self.critic((states,new_actions))\n",
        "        actor_loss = actor_loss[0]\n",
        "    params = self.critic.trainable_variables\n",
        "    grads = tape.gradient(actor_loss,params)\n",
        "    self.actor.optimizer.apply_gradients(zip(grads,params))\n",
        "\n",
        "    if (self.learnCounter % 100 == 0):\n",
        "      self.Update_net_parameters()\n",
        "      self.learnCounter = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8mfQlx_jpTuZ"
      },
      "outputs": [],
      "source": [
        "env.reset()\n",
        "agent_net = {}\n",
        "agent_list = []\n",
        "for a in env.agents:\n",
        "  n_actions = env.action_space(a).n\n",
        "  input_dims = env.observation_space(a).shape[0]\n",
        "  if(a == 'adversary_0'):\n",
        "    agent_Opp = Agent_Opp(input_dims= input_dims ,alpha = 1e-5,env = env , n_actions = n_actions,name = a)\n",
        "  else:\n",
        "    agent =  Agent_Good(input_dims= input_dims ,alpha = 1e-5,env = env , n_actions = n_actions,name = a)\n",
        "    agent_list.append(a)\n",
        "    agent_net[a] = agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "V_VeWxC5qyhr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "outputId": "7caee33b-f624-4ff3-b243-c125a1bccb43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "....saving models....\n",
            "....saving models....\n",
            "....saving models....\n",
            "episode:  0 Opp average score: -25.11 Good average score 11.39\n",
            "....saving models....\n",
            "episode:  1 Opp average score: -17.26 Good average score -3.47\n",
            "episode:  2 Opp average score: -19.48 Good average score 2.68\n",
            "episode:  3 Opp average score: -20.38 Good average score 6.14\n",
            "episode:  4 Opp average score: -19.18 Good average score 4.89\n",
            "episode:  5 Opp average score: -19.98 Good average score 5.97\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-d324d4cb215f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mOpp_score\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0magent_Opp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_transition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0magent_Opp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-fc2920ace181>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mtarget_actions_probs_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_states_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mtarget_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_actions_probs_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mtarget_actions_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mactions_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/distributions/distribution.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, sample_shape, seed, name, **kwargs)\u001b[0m\n\u001b[1;32m   1232\u001b[0m     \"\"\"\n\u001b[1;32m   1233\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_and_control_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1234\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_sample_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_sample_and_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/distributions/distribution.py\u001b[0m in \u001b[0;36m_call_sample_n\u001b[0;34m(self, sample_shape, seed, **kwargs)\u001b[0m\n\u001b[1;32m   1210\u001b[0m         sample_shape, 'sample_shape')\n\u001b[1;32m   1211\u001b[0m     samples = self._sample_n(\n\u001b[0;32m-> 1212\u001b[0;31m         n, seed=seed() if callable(seed) else seed, **kwargs)\n\u001b[0m\u001b[1;32m   1213\u001b[0m     samples = tf.nest.map_structure(\n\u001b[1;32m   1214\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/distributions/categorical.py\u001b[0m in \u001b[0;36m_sample_n\u001b[0;34m(self, n, seed)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteger_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m       draws = tf.random.categorical(\n\u001b[0;32m--> 243\u001b[0;31m           logits_2d, n, dtype=sample_dtype, seed=seed)\n\u001b[0m\u001b[1;32m    244\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m       draws = samplers.categorical(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/random_ops.py\u001b[0m in \u001b[0;36mcategorical\u001b[0;34m(logits, num_samples, dtype, seed, name)\u001b[0m\n\u001b[1;32m    525\u001b[0m   \"\"\"\n\u001b[1;32m    526\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"categorical\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmultinomial_categorical_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/random_ops.py\u001b[0m in \u001b[0;36mmultinomial_categorical_impl\u001b[0;34m(logits, num_samples, dtype, seed)\u001b[0m\n\u001b[1;32m    539\u001b[0m   \u001b[0mseed1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m   return gen_random_ops.multinomial(\n\u001b[0;32m--> 541\u001b[0;31m       logits, num_samples, seed=seed1, seed2=seed2, output_dtype=dtype)\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_random_ops.py\u001b[0m in \u001b[0;36mmultinomial\u001b[0;34m(logits, num_samples, seed, seed2, output_dtype, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m       return multinomial_eager_fallback(\n\u001b[1;32m     58\u001b[0m           \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m           output_dtype=output_dtype, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_random_ops.py\u001b[0m in \u001b[0;36mmultinomial_eager_fallback\u001b[0;34m(logits, num_samples, seed, seed2, output_dtype, name, ctx)\u001b[0m\n\u001b[1;32m     98\u001b[0m   \u001b[0moutput_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_dtype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0m_attr_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs_to_matching_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhalf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   _attrs = (\"seed\", seed, \"seed2\", seed2, \"T\", _attr_T, \"output_dtype\",\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1582\u001b[0m   \u001b[0;31m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1583\u001b[0m   \u001b[0mpreferred_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreferred_dtype\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype_hint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1584\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEagerTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1585\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1586\u001b[0m       \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "n_games = 2500\n",
        "Good_best_score = -1000\n",
        "Opp_best_score = -1000\n",
        "Opp_score_history = []\n",
        "Good_score_history = []\n",
        "Opp_state_ = []\n",
        "Good_state_ = []\n",
        "load_checkpoint = False\n",
        "if load_checkpoint:\n",
        "  for a in agent_list:\n",
        "    agent_net[a].load_models()\n",
        "else:\n",
        "  evaluate = False\n",
        "for i in range(n_games):\n",
        "  Opp_score = 0\n",
        "  Good_score = 0\n",
        "  env.reset(seed = 3)\n",
        "  for agent in env.agent_iter():\n",
        "    state,reward,done,trunc,info = env.last()\n",
        "    if (agent == 'adversary_0'):\n",
        "      if not (done or trunc):\n",
        "        action = agent_Opp.choose_action(state,evaluate)\n",
        "        env.step(action)\n",
        "        Opp_score += reward\n",
        "        agent_Opp.store_transition(state,action,reward,done)\n",
        "        agent_Opp.learn() \n",
        "      else:\n",
        "        env.step(None)\n",
        "    else:\n",
        "      if not (done or trunc):\n",
        "        action = agent_net[agent].choose_action(state,evaluate)\n",
        "        env.step(action)\n",
        "        Good_score += reward\n",
        "        for a in agent_list:\n",
        "          if (a != agent):  \n",
        "            agent_net[agent].learn([agent_net[a]])\n",
        "      else:\n",
        "        env.step(None)\n",
        "\n",
        "  Good_score_history.append(Good_score)\n",
        "  Opp_score_history.append(Opp_score)\n",
        "  Good_avg_score = np.mean(Good_score_history[-100:])\n",
        "  Opp_avg_score = np.mean(Opp_score_history[-100:])\n",
        "\n",
        "  if Good_avg_score > Good_best_score:\n",
        "    Good_best_score = Good_avg_score\n",
        "    if not load_checkpoint:\n",
        "      for a in agent_list:\n",
        "        agent_net[a].save_models()\n",
        "\n",
        "  if Opp_avg_score > Opp_best_score:\n",
        "    Opp_best_score = Opp_avg_score\n",
        "    if not load_checkpoint:\n",
        "      agent_Opp.save_models()\n",
        "  \n",
        "  \n",
        "  print('episode: ', i,'Opp average score: %.2f' % Opp_avg_score,'Good average score %.2f' % Good_avg_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2368h6WqW34X"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import savgol_filter\n",
        "\n",
        "x = np.linspace(0,2*np.pi,100)\n",
        "y = np.sin(x) + np.random.random(100) * 0.2\n",
        "yhat = savgol_filter(Good_score_history, 101, 7) # window size 51, polynomial order 3\n",
        "\n",
        "\n",
        "plt.plot(yhat, color='red')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import savgol_filter\n",
        "\n",
        "x = np.linspace(0,2*np.pi,100)\n",
        "y = np.sin(x) + np.random.random(100) * 0.2\n",
        "yhat = savgol_filter(Opp_score_history, 101, 7) # window size 51, polynomial order 3\n",
        "\n",
        "\n",
        "plt.plot(yhat, color='red')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6TCsxuotEQ3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSGoC__asq3W"
      },
      "outputs": [],
      "source": [
        "print(Good_score_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKeiqFVIPQ3c"
      },
      "outputs": [],
      "source": [
        "print(Opp_score_history)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/adversary_0_actor.zip /content/adversary_0_actor\n",
        "!zip -r /content/agent_0_actor.zip /content/agent_0_actor\n",
        "!zip -r /content/agent_1_actor.zip /content/agent_1_actor"
      ],
      "metadata": {
        "id": "shF3tpIexyuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('/content/agent_1_actor.zip')\n",
        "files.download('/content/agent_0_actor.zip')\n",
        "files.download('/content/adversary_0_actor.zip')"
      ],
      "metadata": {
        "id": "Jzp8ITeIyeCC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}