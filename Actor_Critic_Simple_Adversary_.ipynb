{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luthandonx/Multi-Agent-Reinforcement-Learining/blob/Simple-Adversary/Actor_Critic_Simple_Adversary_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BoeWWpffPWV"
      },
      "outputs": [],
      "source": [
        "!pip install pettingzoo[mpe]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2pYlVprPd2l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model,layers\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "class ActorCriticNetwork(Model):\n",
        "  def __init__(self,n_actions, dims_1 = 256 , dims_2 = 256,\n",
        "               name = 'actor_critc', chkpt_dir = 'tmp/actor_critic'):\n",
        "    super(ActorCriticNetwork, self).__init__()\n",
        "    self.dims_1 = dims_1\n",
        "    self.dims_2 = dims_2\n",
        "    self.n_actions = n_actions\n",
        "    self.model_name = name\n",
        "    self.checkpoint_dir = chkpt_dir\n",
        "    self.checkpoint_file = os.path.join(self.checkpoint_dir, name+'_ac')\n",
        "\n",
        "    self.fc1 = layers.Dense(self.dims_1,activation = 'relu')\n",
        "    self.fc2 = layers.Dense(self.dims_2,activation = 'relu')\n",
        "    self.v = layers.Dense(1,activation = None)\n",
        "    self.po = layers.Dense(n_actions,activation = 'softmax')\n",
        "\n",
        "  def call(self,state):\n",
        "    value = self.fc1(state)\n",
        "    value = self.fc2(value)\n",
        "\n",
        "    v = self.v(value)\n",
        "    po = self.po(value)\n",
        "\n",
        "    return v,po"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjVVvJMuTH2b"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "class Agent:\n",
        "  def __init__(self, alpha = 1e-5, gamma = 0.95, n_actions = 4,name='agent'):\n",
        "    self.gamma = gamma\n",
        "    self.n_actions = n_actions\n",
        "    self.action = None\n",
        "    self.action_space = [i for i in range(self.n_actions)]\n",
        "    self.name = name\n",
        "    self.actor_critic = ActorCriticNetwork(n_actions=n_actions)\n",
        "    self.actor_critic.compile(optimizer = Adam(learning_rate = alpha))\n",
        "    self.fname = 'actor_critic_'+self.name\n",
        "\n",
        "    self.reward_history = []\n",
        "    self.action_history = []\n",
        "    self.obsv_history = []\n",
        "    self.next_obsv_history = []\n",
        "\n",
        "  def choose_action(self,observation):\n",
        "    state = tf.convert_to_tensor([observation])\n",
        "    _,probs = self.actor_critic(state)\n",
        "\n",
        "    action_probs = tfp.distributions.Categorical(probs = probs)\n",
        "    action = action_probs.sample()\n",
        "\n",
        "    self.action = action\n",
        "\n",
        "    return action.numpy()[0]\n",
        "\n",
        "  def save_models(self):\n",
        "    print('saving model.....')\n",
        "    self.actor_critic.save(self.fname)\n",
        "  \n",
        "  def load_models(self):\n",
        "    #print('loading model....')\n",
        "    self.actor_critic = tf.keras.models_load_model(self.fname)\n",
        "\n",
        "  def store_trans(self,action,reward,obsv,obsv_):\n",
        "      self.action_history.append(action)\n",
        "      self.reward_history.append(reward)\n",
        "      self.obsv_history.append(obsv)\n",
        "      self.next_obsv_history.append(obsv_)\n",
        "\n",
        "  def clear_memory(self):\n",
        "    self.reward_history = []\n",
        "    self.action_history = []\n",
        "    self.obsv_history = []\n",
        "    self.next_obsv_history = []\n",
        "\n",
        "  def learn(self,state,reward,state_,done):\n",
        "    state = tf.convert_to_tensor([state],dtype = tf.float32)\n",
        "    state_ = tf.convert_to_tensor([state_],dtype = tf.float32)\n",
        "    reward = tf.convert_to_tensor(reward,dtype = tf.float32)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      state_value, probs = self.actor_critic(state)\n",
        "      state_value_,_ = self.actor_critic(state_)\n",
        "      state_value = tf.squeeze(state_value)\n",
        "      state_value_ = tf.squeeze(state_value_)\n",
        "\n",
        "      action_probs = tfp.distributions.Categorical(probs = probs)\n",
        "      log_probs = action_probs.log_prob(self.action)\n",
        "\n",
        "      delta = reward + self.gamma*state_value_*(1-int(done)) - state_value\n",
        "\n",
        "      actor_loss = -log_probs*delta\n",
        "      critic_loss = delta**2\n",
        "\n",
        "      total_loss = actor_loss + critic_loss\n",
        "\n",
        "    gradients = tape.gradient(total_loss,self.actor_critic.trainable_variables)\n",
        "    self.actor_critic.optimizer.apply_gradients(zip(gradients, self.actor_critic.trainable_variables))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjZt3bDBfG2M"
      },
      "outputs": [],
      "source": [
        "from pettingzoo.mpe import simple_adversary_v2\n",
        "import numpy as np\n",
        "\n",
        "env = simple_adversary_v2.env(N=2,max_cycles = 25 , continuous_actions = False )\n",
        "env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kS-tdY_mf9SG"
      },
      "outputs": [],
      "source": [
        "env.reset()\n",
        "agent_net = {}\n",
        "agent_list = []\n",
        "for a in env.agents:\n",
        "  n_actions = env.action_space(a).n\n",
        "  input_dims = env.observation_space(a).shape[0]\n",
        "  if (a == 'adversary_0'):\n",
        "    opp_agent = Agent(alpha = 1e-5,n_actions = 5,name = a)\n",
        "  else:\n",
        "    good_agent = Agent(alpha = 1e-5,n_actions = 5,name = a)\n",
        "    agent_net[a] = good_agent\n",
        "    agent_list.append(a)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fAngBQwELWlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GfIVgMQgrZe"
      },
      "outputs": [],
      "source": [
        "env.reset()\n",
        "max_episode = 1000\n",
        "good_best_score = -1000\n",
        "opp_best_score = -10000\n",
        "Good_score_history = []\n",
        "Opp_score_history = []\n",
        "good_score = 0\n",
        "opp_score  = 0\n",
        "Opp_state_ = []\n",
        "Good_state_ = []\n",
        "Good_avg_score = []\n",
        "Opp_score_history = []\n",
        "n_games = 1000\n",
        "\n",
        "for i in range(n_games):\n",
        "  env.reset()\n",
        "  good_score = 0\n",
        "  opp_score  = 0\n",
        "  for agent in env.agent_iter():\n",
        "    obsv,reward,done,trunc,_ = env.last()\n",
        "    if not (done or trunc):\n",
        "      if (agent == 'adversary_0'):\n",
        "        action = opp_agent.choose_action(obsv)\n",
        "        opp_score += reward\n",
        "        env.step(action)\n",
        "        obsv_,_,_,_,_ = env.last()\n",
        "        if(len(Opp_state_)>0):\n",
        "          opp_agent.store_trans(action,reward,obsv,Opp_state_[0])\n",
        "          opp_agent.learn(obsv,reward,Opp_state_[0],done)\n",
        "      else:\n",
        "        action = agent_net[agent].choose_action(obsv)\n",
        "        good_score += reward\n",
        "        env.step(action)\n",
        "        state_,_,_,_,_ = env.last()\n",
        "        if (state_.shape == obsv.shape):\n",
        "          agent_net[agent].store_trans(action,reward,obsv,state_)\n",
        "          agent_net[agent].learn(obsv,reward,state_,done)\n",
        "        else:\n",
        "          Opp_state_.append(state_)  \n",
        "    else:\n",
        "      env.step(None)   \n",
        "  \n",
        "  Good_score_history.append(good_score)\n",
        "  Opp_score_history.append(opp_score)\n",
        "\n",
        "  Good_avg_score = np.mean(Good_score_history[-100:])\n",
        "  Opp_avg_score = np.mean(Opp_score_history[-100:])\n",
        "\n",
        "  if (Good_avg_score > good_best_score):\n",
        "    good_best_score = Good_avg_score\n",
        "    for a in agent_list:\n",
        "       agent_net[a].save_models()\n",
        "\n",
        "  if (Opp_avg_score > opp_best_score):\n",
        "    opp_best_score = Opp_avg_score\n",
        "    opp_agent.save_models()\n",
        "  \n",
        "  print('episode: ', i ,' Opp avg score:', Opp_avg_score,'Good avg score :',Good_avg_score)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/actor_critic_adversary_0.zip /content/actor_critic_adversary_0\n",
        "!zip -r /content/actor_critic_agent_1.zip /content/actor_critic_agent_1\n",
        "!zip -r /content/actor_critic_agent_0.zip /content/actor_critic_agent_0"
      ],
      "metadata": {
        "id": "q_L90rSkfW25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n"
      ],
      "metadata": {
        "id": "EekTuwoTf9iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !zip -r /content/actor_critic_agent_0.zip /content/actor_critic_agent_0\n",
        "# !zip -r /content/actor_critic_agent_1.zip /content/actor_critic_agent_1\n",
        "files.download('/content/actor_critic_adversary_0.zip')\n",
        "files.download('/content/actor_critic_agent_0.zip')\n",
        "files.download('/content/actor_critic_agent_1.zip')"
      ],
      "metadata": {
        "id": "uXYbwxr6kb6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4q7w_pkNZQX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import savgol_filter\n",
        "\n",
        "x = np.linspace(0,2*np.pi,100)\n",
        "y = np.sin(x) + np.random.random(100) * 0.2\n",
        "yhat = savgol_filter(Good_score_history, 101, 7) # window size 51, polynomial order 3\n",
        "opphat = savgol_filter(Opp_score_history, 101, 7) \n",
        "\n",
        "plt.plot(yhat, color='red')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THSjSQ9jRorP"
      },
      "outputs": [],
      "source": [
        "print(Good_score_history)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Opp_score_history)"
      ],
      "metadata": {
        "id": "LlFGbuxpPxt2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyPoLI2rGZR/QolPYnh56DSZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}